{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.tree_util import tree_flatten, tree_unflatten, tree_structure\n",
    "import jax_dataclasses as jdc\n",
    "import diffrax\n",
    "import chex\n",
    "from functools import partial\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from exciting_environments import spaces\n",
    "from dataclasses import fields\n",
    "\n",
    "class CoreEnvironment(ABC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Core Structure of provided Environments.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            batch_size: int,\n",
    "            physical_constraints,\n",
    "            action_constraints,\n",
    "            static_params,\n",
    "            tau: float = 1e-4,\n",
    "            solver=diffrax.Euler(),\n",
    "            reward_func=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size(int): Number of training examples utilized in one iteration.\n",
    "            physical_constraints(jdc.pytree_dataclass): Constraints of physical states of the environment.\n",
    "            action_constraints(jdc.pytree_dataclass): Constraints of actions.\n",
    "            static_params(jdc.pytree_dataclass): Parameters of environment which do not change during simulation.\n",
    "            tau(float): Duration of one control step in seconds. Default: 1e-4.\n",
    "            solver(diffrax.solver): Solver used to compute states for next step.\n",
    "            reward_func(function): Reward function for training. Needs observation vector, action and action_constraints as Parameters. \n",
    "                                    Default: None (default_reward_func from class) \n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self._solver = solver\n",
    "        self.env_properties = self.EnvProperties(\n",
    "            physical_constraints=physical_constraints, action_constraints=action_constraints, static_params=static_params)\n",
    "        self.in_axes_env_properties = self.create_in_axes_dataclass(self.env_properties)\n",
    "        if reward_func:\n",
    "            if self._test_rew_func(reward_func):\n",
    "                self.reward_func = reward_func\n",
    "        else:\n",
    "            self.reward_func = self.default_reward_func\n",
    "\n",
    "    @property\n",
    "    def default_reward_function(self):\n",
    "        \"\"\"Returns the default reward function for the given environment.\"\"\"\n",
    "        return self.default_reward_func\n",
    "\n",
    "    @abstractmethod\n",
    "    @jdc.pytree_dataclass\n",
    "    class PhysicalStates:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    @jdc.pytree_dataclass\n",
    "    class Optional:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    @jdc.pytree_dataclass\n",
    "    class StaticParams:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    @jdc.pytree_dataclass\n",
    "    class Actions:\n",
    "        pass\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class States:\n",
    "        \"\"\"Dataclass used for simulation which contains environment specific dataclasses.\"\"\"\n",
    "        physical_states: jdc.pytree_dataclass\n",
    "        PRNGKey: jax.Array\n",
    "        optional: jdc.pytree_dataclass\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class EnvProperties:\n",
    "        \"\"\"Dataclass used for simulation which contains environment specific dataclasses.\"\"\"\n",
    "        physical_constraints: jdc.pytree_dataclass\n",
    "        action_constraints: jdc.pytree_dataclass\n",
    "        static_params: jdc.pytree_dataclass\n",
    "\n",
    "    def create_in_axes_dataclass(self,dataclass):\n",
    "        with jdc.copy_and_mutate(dataclass,validate=False) as dataclass_in_axes:\n",
    "            for field in fields(dataclass_in_axes):\n",
    "                name=field.name\n",
    "                value=getattr(dataclass_in_axes, name)\n",
    "                if jdc.is_dataclass(value):\n",
    "                    setattr(dataclass_in_axes,name,self.create_in_axes_dataclass(value))\n",
    "                elif jnp.isscalar(value):\n",
    "                    setattr(dataclass_in_axes,name,None)\n",
    "                else:\n",
    "                    assert len(\n",
    "                        value) == self.batch_size, f\"{name} is expected to be a scalar a pytree_dataclass or a jnp.Array with len(jnp.Array)=batch_size={self.batch_size}\"\n",
    "                    setattr(dataclass_in_axes,name,0)\n",
    "        return dataclass_in_axes\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def step(self, states, action, env_properties):\n",
    "        \"\"\"Computes one simulation step for one batch.\n",
    "\n",
    "        Args:\n",
    "            states: The states from which to calculate states for the next step.\n",
    "            action: The action to apply to the environment.\n",
    "            env_properties: Contains action/state constraints and static parameter.\n",
    "\n",
    "        Returns:\n",
    "            Multiple Outputs:\n",
    "\n",
    "            observation: The gathered observation.\n",
    "            reward: Amount of reward received for the last step.\n",
    "            terminated: Flag, indicating if Agent has reached the terminal state.\n",
    "            truncated: Flag, e.g. indicating if state has gone out of bounds.\n",
    "            states: New states for the next step.\n",
    "        \"\"\"\n",
    "\n",
    "        # ode step\n",
    "        states = self._ode_solver_step(\n",
    "            states, action, env_properties.static_params\n",
    "        )\n",
    "\n",
    "        # observation\n",
    "        obs = self.generate_observation(\n",
    "            states, env_properties.physical_constraints\n",
    "        )\n",
    "\n",
    "        # reward\n",
    "        reward = self.reward_func(\n",
    "            obs, action, env_properties.action_constraints\n",
    "        )\n",
    "\n",
    "        # bound check\n",
    "        truncated = self.generate_truncated(\n",
    "            states, env_properties.physical_constraints\n",
    "        )\n",
    "\n",
    "        terminated = self.generate_terminated(\n",
    "            states, reward\n",
    "        )\n",
    "\n",
    "        return obs, reward, terminated, truncated, states\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def vmap_step(self, action, states):\n",
    "        \"\"\"JAX jit compiled and vmapped step for batch_size of environment.\n",
    "\n",
    "        Args:\n",
    "            states: The states from which to calculate states for the next step.\n",
    "            action: The action to apply to the environment.\n",
    "            env_properties: Contains action/state constraints and static parameters.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            Multiple Outputs:\n",
    "\n",
    "            observation: The gathered observations (shape=(batch_size,obs_dim)).\n",
    "            reward: Amount of reward received for the last step (shape=(batch_size,1)).\n",
    "            terminated: Flag, indicating if Agent has reached the terminal state (shape=(batch_size,1)).\n",
    "            truncated: Flag, indicating if state has gone out of bounds (shape=(batch_size,states_dim)).\n",
    "            states: New states for the next step.\n",
    "\n",
    "        \"\"\"\n",
    "        # vmap single operations\n",
    "        obs, reward, terminated, truncated, states = jax.vmap(self.step, in_axes=(0, 0, self.in_axes_env_properties))(\n",
    "            states, action, self.env_properties\n",
    "        )\n",
    "\n",
    "        return obs, reward, terminated, truncated, states\n",
    "\n",
    "    # @partial(jax.jit, static_argnums=0)\n",
    "    # def vmap_simulate_ahead(self, actions, init_states, init_obs):\n",
    "\n",
    "    #     def body_fun(carry, action):\n",
    "    #         obs, states = carry\n",
    "\n",
    "    #         obs, _, _, _, states = self.vmap_step(\n",
    "    #             action.reshape(-1, 1),\n",
    "    #             states\n",
    "    #         )\n",
    "    #         return (obs, states), obs\n",
    "\n",
    "    #     (_, _), observations = jax.lax.scan(\n",
    "    #         body_fun, (init_obs, init_states), actions.T)\n",
    "    #     observations = jnp.concatenate(\n",
    "    #         [init_obs[None, :], observations], axis=0)\n",
    "\n",
    "    #     return observations\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def obs_description(self):\n",
    "        \"\"\"Returns a list of state names of all states in the observation.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def default_reward_func(self, obs, action):\n",
    "        \"\"\"Returns the default RewardFunction of the environment.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_observation(self, states):\n",
    "        \"\"\"Returns observation.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_truncated(self, states):\n",
    "        \"\"\"Returns truncated information.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_terminated(self, states, reward):\n",
    "        \"\"\"Returns terminated information.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def _ode_solver_step(self, states_norm, action_norm, state_normalizer,  action_normalizer, params):\n",
    "        \"\"\"Computes states by simulating one step.\n",
    "\n",
    "        Args:\n",
    "            states: The states from which to calculate states for the next step.\n",
    "            action: The action to apply to the environment.\n",
    "            static_params: Parameter of the environment, that do not change over time.\n",
    "\n",
    "        Returns:\n",
    "            states: The computed states after the one step simulation.\n",
    "        \"\"\"\n",
    "\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self, rng: chex.PRNGKey = None, initial_states: jdc.pytree_dataclass = None):\n",
    "        \"\"\"Resets environment to default or passed initial values.\"\"\"\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.tree_util import tree_flatten, tree_unflatten, tree_structure\n",
    "import jax_dataclasses as jdc\n",
    "import chex\n",
    "from functools import partial\n",
    "import diffrax\n",
    "from exciting_environments import core_env\n",
    "\n",
    "\n",
    "class Pendulum(CoreEnvironment):\n",
    "    \"\"\"\n",
    "    State Variables:\n",
    "        ``['theta', 'omega']``\n",
    "\n",
    "    Action Variable:\n",
    "        ``['torque']``\n",
    "\n",
    "    Initial State:\n",
    "        Unless chosen otherwise, theta equals pi and omega is set to zero.\n",
    "\n",
    "    Example:\n",
    "        >>> import jax\n",
    "        >>> import exciting_environments as excenvs\n",
    "        >>> from exciting_environments import GymWrapper\n",
    "        >>> \n",
    "        >>> # Create the environment\n",
    "        >>> pend=excenv.Pendulum(batch_size=4,action_constraints={\"torque\":10})\n",
    "        >>> \n",
    "        >>> # Use GymWrapper for Simulation (optional)\n",
    "        >>> gym_pend=GymWrapper(env=pend)\n",
    "        >>> \n",
    "        >>> # Reset the environment with default initial values\n",
    "        >>> gym_pend.reset()\n",
    "        >>> \n",
    "        >>> # Perform step\n",
    "        >>> obs,reward,terminated,truncated,info= gym_pend.step(action=jnp.ones(4).reshape(-1,1))\n",
    "        >>> \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 8,\n",
    "        physical_constraints: dict = None,\n",
    "        action_constraints: dict = None,\n",
    "        static_params: dict = None,\n",
    "        solver=diffrax.Euler(),\n",
    "        reward_func=None,\n",
    "        tau: float = 1e-4,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size(int): Number of training examples utilized in one iteration. Default: 8\n",
    "            physical_constraints(dict): Constraints of physical states of the environment.\n",
    "                theta(float): Rotation angle. Default: jnp.pi\n",
    "                omega(float): Angular velocity. Default: 10\n",
    "            action_constraints(dict): Constraints of actions.\n",
    "                torque(float): Maximum torque that can be applied to the system as action. Default: 20 \n",
    "            static_params(dict): Parameters of environment which do not change during simulation.\n",
    "                l(float): Length of the pendulum. Default: 1\n",
    "                m(float): Mass of the pendulum tip. Default: 1\n",
    "                g(float): Gravitational acceleration. Default: 9.81\n",
    "            solver(diffrax.solver): Solver used to compute states for next step.\n",
    "            reward_func(function): Reward function for training. Needs observation vector, action and action_constraints as Parameters. \n",
    "                                    Default: None (default_reward_func from class)\n",
    "            tau(float): Duration of one control step in seconds. Default: 1e-4.\n",
    "\n",
    "        Note: Attributes of physical_constraints, action_constraints and static_params can also be passed as jnp.Array with the length of the batch_size to set different values per batch.  \n",
    "        \"\"\"\n",
    "\n",
    "        if not physical_constraints:\n",
    "            physical_constraints = {\"theta\": jnp.pi, \"omega\": 10}\n",
    "        if not action_constraints:\n",
    "            action_constraints = {\"torque\": 20}\n",
    "\n",
    "        if not static_params:\n",
    "            static_params = {\"g\": 9.81, \"l\": 2, \"m\": 1}\n",
    "\n",
    "        physical_constraints = self.PhysicalStates(**physical_constraints)\n",
    "        action_constraints = self.Actions(**action_constraints)\n",
    "        static_params = self.StaticParams(**static_params)\n",
    "\n",
    "        super().__init__(batch_size, physical_constraints, action_constraints, static_params, tau=tau,\n",
    "                         solver=solver, reward_func=reward_func)\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class PhysicalStates:\n",
    "        \"\"\"Dataclass containing the physical states of the environment.\"\"\"\n",
    "        theta: jax.Array\n",
    "        omega: jax.Array\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class Optional:\n",
    "        \"\"\"Dataclass containing additional information for simulation.\"\"\"\n",
    "        something: jax.Array\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class StaticParams:\n",
    "        \"\"\"Dataclass containing the static parameters of the environment.\"\"\"\n",
    "        g: jax.Array\n",
    "        l: jax.Array\n",
    "        m: jax.Array\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class Actions:\n",
    "        \"\"\"Dataclass containing the actions, that can be applied to the environment.\"\"\"\n",
    "        torque: jax.Array\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _ode_solver_step(self, states, action, static_params):\n",
    "        \"\"\"Computes states by simulating one step.\n",
    "\n",
    "        Args:\n",
    "            states: The states from which to calculate states for the next step.\n",
    "            action: The action to apply to the environment.\n",
    "            static_params: Parameter of the environment, that do not change over time.\n",
    "\n",
    "        Returns:\n",
    "            states: The computed states after the one step simulation.\n",
    "        \"\"\"\n",
    "\n",
    "        env_states = states.physical_states\n",
    "        args = (action, static_params)\n",
    "\n",
    "        def vector_field(t, y, args):\n",
    "            theta, omega = y\n",
    "            action, params = args\n",
    "            d_omega = (action[0]+params.l*params.m*params.g\n",
    "                       * jnp.sin(theta)) / (params.m * (params.l)**2)\n",
    "            d_theta = omega\n",
    "            d_y = d_theta, d_omega\n",
    "            return d_y\n",
    "\n",
    "        term = diffrax.ODETerm(vector_field)\n",
    "        t0 = 0\n",
    "        t1 = self.tau\n",
    "        y0 = tuple([env_states.theta, env_states.omega])\n",
    "        env_state = self._solver.init(term, t0, t1, y0, args)\n",
    "        y, _, _, env_state, _ = self._solver.step(\n",
    "            term, t0, t1, y0, args, env_state, made_jump=False)\n",
    "\n",
    "        theta_k1 = y[0]\n",
    "        omega_k1 = y[1]\n",
    "        theta_k1 = ((theta_k1+jnp.pi) % (2*jnp.pi))-jnp.pi\n",
    "\n",
    "        phys = self.PhysicalStates(\n",
    "            theta=theta_k1, omega=omega_k1)\n",
    "        opt = None  # Optional(something=...)\n",
    "        return self.States(physical_states=phys, PRNGKey=None, optional=None)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def init_states(self):\n",
    "        \"\"\"Returns default initial states for all batches.\"\"\"\n",
    "        phys = self.PhysicalStates(theta=jnp.full(\n",
    "            self.batch_size, jnp.pi), omega=jnp.zeros(self.batch_size))\n",
    "        opt = None  # self.Optional(something=jnp.zeros(self.batch_size))\n",
    "        return self.States(physical_states=phys, PRNGKey=None, optional=opt)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def default_reward_func(self, obs, action, action_constraints):\n",
    "        \"\"\"Returns reward for one batch.\"\"\"\n",
    "        reward = ((obs[0])**2 + 0.1*(obs[1])**2\n",
    "                  + 0.1 * (action[0]/action_constraints.torque)**2)\n",
    "        return jnp.array([reward])\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_observation(self, states, physical_constraints):\n",
    "        \"\"\"Returns observation for one batch.\"\"\"\n",
    "        obs = jnp.hstack((\n",
    "            states.physical_states.theta / physical_constraints.theta,\n",
    "            states.physical_states.omega / physical_constraints.omega,\n",
    "        ))\n",
    "        return obs\n",
    "\n",
    "    @property\n",
    "    def obs_description(self):\n",
    "        return np.array([\"theta\", \"omega\"])\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_truncated(self, states, physical_constraints):\n",
    "        \"\"\"Returns truncated information for one batch.\"\"\"\n",
    "        _states = jnp.hstack((\n",
    "            states.physical_states.theta / physical_constraints.theta,\n",
    "            states.physical_states.theta / physical_constraints.omega,\n",
    "        ))\n",
    "        return jnp.abs(_states) > 1\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_terminated(self, states, reward):\n",
    "        \"\"\"Returns terminated information for one batch.\"\"\"\n",
    "        return reward == 0\n",
    "\n",
    "    def reset(self, rng: chex.PRNGKey = None, initial_states: jdc.pytree_dataclass = None):\n",
    "        \"\"\"Resets environment to default or passed initial states.\"\"\"\n",
    "        if initial_states is not None:\n",
    "            assert tree_structure(self.init_states()) == tree_structure(initial_states), (\n",
    "                f\"initial_states should have the same dataclass structure as self.init_states()\"\n",
    "            )\n",
    "            states = initial_states\n",
    "        else:\n",
    "            states = self.init_states()\n",
    "\n",
    "        obs = jax.vmap(self.generate_observation, in_axes=(0, self.in_axes_env_properties.physical_constraints))(\n",
    "            states, self.env_properties.physical_constraints\n",
    "        )\n",
    "\n",
    "        return obs, states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pend=Pendulum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[65],\n",
       "       [65],\n",
       "       [65],\n",
       "       [65]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.tile(jnp.array([65]), (4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exciting_environments import GymWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pend=Pendulum(batch_size=5,action_constraints={\"torque\":jnp.array([10,20,30,40,50])})\n",
    "gym_pend=GymWrapper(env=pend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym_pend.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[-1.0000000e+00,  2.4999996e-05],\n",
       "        [-1.0000000e+00,  4.9999999e-05],\n",
       "        [-1.0000000e+00,  7.4999996e-05],\n",
       "        [-1.0000000e+00,  9.9999997e-05],\n",
       "        [-1.0000000e+00,  1.2500001e-04]], dtype=float32),\n",
       " Array([[1.1],\n",
       "        [1.1],\n",
       "        [1.1],\n",
       "        [1.1],\n",
       "        [1.1]], dtype=float32),\n",
       " Array([[False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False]], dtype=bool),\n",
       " Array([[False, False],\n",
       "        [False, False],\n",
       "        [False, False],\n",
       "        [False, False],\n",
       "        [False, False]], dtype=bool))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym_pend.step(action=jnp.ones(5).reshape(-1,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
