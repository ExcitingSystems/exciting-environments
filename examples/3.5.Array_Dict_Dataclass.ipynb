{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import time\n",
    "import chex\n",
    "import jax\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import diffrax\n",
    "from collections import OrderedDict\n",
    "from flax.core import FrozenDict\n",
    "import jax_dataclasses as jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import chex\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from exciting_environments import spaces\n",
    "import diffrax\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class CoreEnvironment(ABC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Core Structure of provided Environments.\n",
    "\n",
    "    State Variables:\n",
    "        Each environment has got a list of state variables that are defined by the physical system represented.\n",
    "\n",
    "        Example:\n",
    "            ``['theta', 'omega']``\n",
    "\n",
    "    Action Variable:\n",
    "        Each environment has got an action which is applied to the physical system represented.\n",
    "\n",
    "        Example:\n",
    "            ``['torque']``\n",
    "\n",
    "    Observation Space(State Space):\n",
    "        Type: Box()\n",
    "            The Observation Space is nothing but the State Space of the pyhsical system.\n",
    "            This Space is a normalized, continious, multidimensional box in [-1,1].\n",
    "\n",
    "    Action Space:\n",
    "        Type: Box()\n",
    "            The action space of the environments are the action spaces of the physical systems.\n",
    "            This Space is a continious, multidimensional box. \n",
    "\n",
    "\n",
    "    Initial State:\n",
    "        Initial state values depend on the physical system.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int, tau: float = 1e-4, solver=diffrax.Euler(), reward_func=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size(int): Number of training examples utilized in one iteration.\n",
    "            tau(float): Duration of one control step in seconds. Default: 1e-4.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self._solver = solver\n",
    "\n",
    "        if reward_func:\n",
    "            if self._test_rew_func(reward_func):\n",
    "                self.reward_func = reward_func\n",
    "        else:\n",
    "            self.reward_func = self.default_reward_func\n",
    "\n",
    "    # @property\n",
    "    # def batch_size(self):\n",
    "    #     \"\"\"Returns the batch size of the environment setup.\"\"\"\n",
    "    #     return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def default_reward_function(self):\n",
    "        \"\"\"Returns the default reward function for the given environment.\"\"\"\n",
    "        return self.default_reward_func\n",
    "\n",
    "    # @batch_size.setter\n",
    "    # def batch_size(self, batch_size):\n",
    "    #     # If batchsize change, update the corresponding dimension\n",
    "    #     self._batch_size = batch_size\n",
    "\n",
    "    def sim_paras(self, static_params_, env_state_constraints_, env_max_actions_):\n",
    "        \"\"\"Creates or updates static parameters to fit batch_size.\n",
    "\n",
    "        Creates/Updates:\n",
    "            params : Model Parameters.\n",
    "        \"\"\"\n",
    "        static_params = static_params_.copy()\n",
    "        for key, value in static_params.items():\n",
    "            if jnp.isscalar(value):\n",
    "                static_params[key]=(jnp.full((self.batch_size), value))\n",
    "                # self.static_para_dims[key] = None\n",
    "            # elif jnp.all(value == value[0]):\n",
    "            #     self.static_params[key] = jnp.full(\n",
    "            #         (self.batch_size, 1), value[0])\n",
    "            else:\n",
    "                assert len(\n",
    "                    value) == self.batch_size, f\"{key} is expected to be a scalar or a list with len(list)=batch_size\"\n",
    "                static_params[key]=(jnp.array(value))\n",
    "                # self.static_para_dims[key] = 0\n",
    "\n",
    "        env_state_constraints = env_state_constraints_.copy()\n",
    "        env_state_constraints_ar=[]\n",
    "        for key, value in env_state_constraints.items():\n",
    "            if jnp.isscalar(value):\n",
    "                env_state_constraints_ar.append(jnp.full((self.batch_size), value))\n",
    "                # self.static_para_dims[key] = None\n",
    "            # elif jnp.all(value == value[0]):\n",
    "            #     self.static_params[key] = jnp.full(\n",
    "            #         (self.batch_size, 1), value[0])\n",
    "            else:\n",
    "                assert len(\n",
    "                    value) == self.batch_size, f\"Constraint of {key} is expected to be a scalar or a list with len(list)=batch_size\"\n",
    "                env_state_constraints_ar.append(jnp.array(value))\n",
    "                # self.static_para_dims[key] = 0\n",
    "\n",
    "        env_max_actions = env_max_actions_.copy()\n",
    "        env_max_actions_ar = []\n",
    "        for key, value in env_max_actions.items():\n",
    "            if jnp.isscalar(value):\n",
    "                env_max_actions_ar.append(jnp.full((self.batch_size), value))\n",
    "                # self.static_para_dims[key] = None\n",
    "            # elif jnp.all(value == value[0]):\n",
    "            #     self.static_params[key] = jnp.full(\n",
    "            #         (self.batch_size, 1), value[0])\n",
    "            else:\n",
    "                assert len(\n",
    "                    value) == self.batch_size, f\"Constraint of {key} is expected to be a scalar or a list with len(list)=batch_size\"\n",
    "                env_max_actions_ar.append(jnp.array(value))\n",
    "                # self.static_para_dims[key] = 0\n",
    "\n",
    "        return static_params, jnp.array(env_state_constraints_ar).T, jnp.array(env_max_actions_ar).T\n",
    "\n",
    "    # def solver(self):\n",
    "    #     \"\"\"Returns the current solver of the environment setup.\"\"\"\n",
    "    #     return self._solver\n",
    "\n",
    "    # @solver.setter\n",
    "    # def solver(self, solver):\n",
    "    #     # TODO:check if solver exists in diffrax ?\n",
    "    #     self._solver = solver\n",
    "\n",
    "    def _test_rew_func(self, func):\n",
    "        \"\"\"Checks if passed reward function is compatible with given environment.\n",
    "\n",
    "        Args:\n",
    "            func(function): Reward function to test.\n",
    "\n",
    "        Returns:\n",
    "            compatible(bool): Environment compatibility.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            out = func(\n",
    "                jnp.zeros([self.batch_size, int(len(self.obs_description))]))\n",
    "        except:\n",
    "            raise Exception(\n",
    "                \"Reward function should be using obs matrix as only parameter\")\n",
    "        try:\n",
    "            if out.shape != (self.batch_size, 1):\n",
    "                raise Exception(\n",
    "                    \"Reward function should be returning vector in shape (batch_size,1)\")\n",
    "        except:\n",
    "            raise Exception(\n",
    "                \"Reward function should be returning vector in shape (batch_size,1)\")\n",
    "        return True\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def step(self, action, states):\n",
    "        \"\"\"Addtional function in step execution to enable JAX jit.\n",
    "\n",
    "        Args:\n",
    "            states(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "            action_norm(ndarray(float)): Action Matrix (shape=(batch_size,actions)).\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            Multiple Outputs:\n",
    "\n",
    "            observation(ndarray(float)): Observation/State Matrix (shape=(batch_size,states)).\n",
    "\n",
    "            reward(ndarray(float)): Amount of reward received for the last step (shape=(batch_size,1)).\n",
    "\n",
    "            terminated(bool): Flag, indicating if Agent has reached the terminal state.\n",
    "\n",
    "            truncated(ndarray(bool)): Flag, indicating if state has gone out of bounds (shape=(batch_size,states)).\n",
    "\n",
    "            {}: An empty dictionary for consistency with the OpenAi Gym interface.\n",
    "\n",
    "        \"\"\"\n",
    "        # ode step\n",
    "        states = jax.vmap(self._ode_exp_euler_step)(\n",
    "            states, action, self.static_params)\n",
    "\n",
    "        # observation\n",
    "        # print(states)\n",
    "        # print(self.env_state_constraints)\n",
    "        #obs = jax.vmap(self.generate_observation)(\n",
    "            #states, self.env_state_constraints)\n",
    "        # reward\n",
    "        #reward = jax.vmap(self.reward_func)(\n",
    "            #obs, action, self.env_max_actions).reshape(-1, 1)\n",
    "\n",
    "        # bound check\n",
    "        #truncated = jax.vmap(self.generate_truncated)(\n",
    "            #states, self.env_state_constraints)\n",
    "        #terminated = jax.vmap(self.generate_terminated)(states, reward)\n",
    "\n",
    "        #return obs, reward, terminated, truncated, states\n",
    "        return {},{},{},{},states\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def obs_description(self):\n",
    "        \"\"\"Returns a list of state names of all states in the observation (equal to state space).\"\"\"\n",
    "        return self.states_description\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def default_reward_func(self, obs, action):\n",
    "        \"\"\"Returns the default RewardFunction of the environment.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_observation(self, states):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return states\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_truncated(self, states):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_terminated(self, states, reward):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def _ode_exp_euler_step(self, states_norm, action_norm, state_normalizer,  action_normalizer, params):\n",
    "        \"\"\"Implementation of the system equations in the class with Explicit Euler.\n",
    "\n",
    "        Args:\n",
    "            states_norm(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "            action_norm(ndarray(float)): Action Matrix (shape=(batch_size,actions)).\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            states(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self, initial_values: jnp.ndarray = jnp.array([])):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import diffrax\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class Pendulum(CoreEnvironment):\n",
    "    \"\"\"\n",
    "    State Variables:\n",
    "        ``['theta', 'omega']``\n",
    "\n",
    "    Action Variable:\n",
    "        ``['torque']``\n",
    "\n",
    "    Observation Space (State Space):\n",
    "        Box(low=[-1, -1], high=[1, 1])    \n",
    "\n",
    "    Action Space:\n",
    "        Box(low=-1, high=1)\n",
    "\n",
    "    Initial State:\n",
    "        Unless chosen otherwise, theta equals 1(normalized to pi) and omega is set to zero.\n",
    "\n",
    "    Example:\n",
    "        >>> import jax\n",
    "        >>> import exciting_environments as excenvs\n",
    "        >>> \n",
    "        >>> # Create the environment\n",
    "        >>> env= excenvs.make('Pendulum-v0',batch_size=2,l=2,m=4)\n",
    "        >>> \n",
    "        >>> # Reset the environment with default initial values\n",
    "        >>> env.reset()\n",
    "        >>> \n",
    "        >>> # Sample a random action\n",
    "        >>> action = env.action_space.sample(jax.random.PRNGKey(6))\n",
    "        >>> \n",
    "        >>> # Perform step\n",
    "        >>> obs,reward,terminated,truncated,info= env.step(action)\n",
    "        >>> \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int = 8, l: float = 1, m: float = 1,  env_max_actions: list = {\"torque\": 20}, solver=diffrax.Euler(), reward_func=None, g: float = 9.81, tau: float = 1e-4, env_state_constraints: dict = {\"theta\": np.pi, \"omega\": 10}):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size(int): Number of training examples utilized in one iteration. Default: 8\n",
    "            l(float): Length of the pendulum. Default: 1\n",
    "            m(float): Mass of the pendulum tip. Default: 1\n",
    "            max_torque(float): Maximum torque that can be applied to the system as action. Default: 20 \n",
    "            reward_func(function): Reward function for training. Needs Observation-Matrix and Action as Parameters. \n",
    "                                    Default: None (default_reward_func from class) \n",
    "            g(float): Gravitational acceleration. Default: 9.81\n",
    "            tau(float): Duration of one control step in seconds. Default: 1e-4.\n",
    "            constraints(list): Constraints for state ['omega'] (list with length 1). Default: [10]\n",
    "\n",
    "        Note: l,m and max_torque can also be passed as lists with the length of the batch_size to set different parameters per batch. In addition to that constraints can also be passed as a list of lists with length 1 to set different constraints per batch.  \n",
    "        \"\"\"\n",
    "        self.env_states_name = [\"theta\", \"omega\"]\n",
    "        self.env_actions_name = [\"torque\"]\n",
    "\n",
    "        self.env_states_initials = [jnp.pi,0]\n",
    "\n",
    "        super().__init__(batch_size=batch_size, tau=tau,\n",
    "                         solver=solver, reward_func=reward_func)\n",
    "\n",
    "        self.static_params, self.env_state_constraints, self.env_max_actions = self.sim_paras(\n",
    "            {\"l\": l, \"m\": m, \"g\": g}, env_state_constraints, env_max_actions)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _ode_exp_euler_step(self, states, action, static_params):\n",
    "\n",
    "        env_states = states\n",
    "        args = (action, static_params)\n",
    "\n",
    "        def vector_field(t, y, args):\n",
    "            theta, omega = y\n",
    "            action, params = args\n",
    "            d_omega = (action[0]+params[\"l\"]*params[\"m\"]*params[\"g\"]\n",
    "                       * jnp.sin(theta)) / (params[\"m\"] * (params[\"l\"])**2)\n",
    "            d_theta = omega\n",
    "            d_y = d_theta, d_omega#[0]  # d_theta, d_omega\n",
    "            return d_y\n",
    "\n",
    "        term = diffrax.ODETerm(vector_field)\n",
    "        t0 = 0\n",
    "        t1 = self.tau\n",
    "        y0 = tuple([env_states[0], env_states[1]])\n",
    "        env_state = self._solver.init(term, t0, t1, y0, args)\n",
    "        y, _, _, env_state, _ = self._solver.step(\n",
    "            term, t0, t1, y0, args, env_state, made_jump=False)\n",
    "\n",
    "        theta_k1 = y[0]\n",
    "        omega_k1 = y[1]\n",
    "        theta_k1 = ((theta_k1+jnp.pi) % (2*jnp.pi))-jnp.pi\n",
    "\n",
    "        env_states_k1 = jnp.hstack((\n",
    "            theta_k1,\n",
    "            omega_k1,\n",
    "        ))\n",
    "        #env_states_k1 = dict([(\"theta\", theta_k1), (\"omega\", omega_k1)]) #Ordered\n",
    "\n",
    "        # env_states_k1_norm = env_states_k1/env_state_normalizer\n",
    "\n",
    "        return env_states_k1\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def default_reward_func(self, obs, action, env_max_actions):\n",
    "        return (obs[0])**2 + 0.1*(obs[1])**2 + 0.1*(action[\"torque\"]/env_max_actions[\"torque\"])**2\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_observation(self, states, env_state_constraints):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return (jnp.array(list(states.values()))*(jnp.array(list(env_state_constraints.values())))**(-1)).T  #\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_truncated(self, states, env_state_constraints):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return jnp.abs((jnp.array(list(states.values()))/jnp.array(list(env_state_constraints.values()))).T) > 1\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_terminated(self, states, reward):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return reward == 0\n",
    "\n",
    "    @property\n",
    "    def obs_description(self):\n",
    "        return self.env_states_name\n",
    "\n",
    "    def reset(self, initial_values: jnp.ndarray = jnp.array([])):\n",
    "        # TODO\n",
    "        # if initial_values.any() != False:\n",
    "        #     assert initial_values.shape[\n",
    "        #         0] == self.batch_size, f\"number of rows is expected to be batch_size, got: {initial_values.shape[0]}\"\n",
    "        #     assert initial_values.shape[1] == len(\n",
    "        #         self.obs_description), f\"number of columns is expected to be amount obs_entries: {len(self.obs_description)}, got: {initial_values.shape[0]}\"\n",
    "        #     states = initial_values\n",
    "        # else:\n",
    "        #     states = jnp.tile(\n",
    "        #         jnp.array(self.env_state_initials), (self.batch_size, 1))\n",
    "\n",
    "        # obs = self.generate_observation(states)\n",
    "\n",
    "        return  # obs, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import chex\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from exciting_environments import spaces\n",
    "import diffrax\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class CoreEnvironment3(ABC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Core Structure of provided Environments.\n",
    "\n",
    "    State Variables:\n",
    "        Each environment has got a list of state variables that are defined by the physical system represented.\n",
    "\n",
    "        Example:\n",
    "            ``['theta', 'omega']``\n",
    "\n",
    "    Action Variable:\n",
    "        Each environment has got an action which is applied to the physical system represented.\n",
    "\n",
    "        Example:\n",
    "            ``['torque']``\n",
    "\n",
    "    Observation Space(State Space):\n",
    "        Type: Box()\n",
    "            The Observation Space is nothing but the State Space of the pyhsical system.\n",
    "            This Space is a normalized, continious, multidimensional box in [-1,1].\n",
    "\n",
    "    Action Space:\n",
    "        Type: Box()\n",
    "            The action space of the environments are the action spaces of the physical systems.\n",
    "            This Space is a continious, multidimensional box. \n",
    "\n",
    "\n",
    "    Initial State:\n",
    "        Initial state values depend on the physical system.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int, tau: float = 1e-4, solver=diffrax.Euler(), reward_func=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size(int): Number of training examples utilized in one iteration.\n",
    "            tau(float): Duration of one control step in seconds. Default: 1e-4.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self._solver = solver\n",
    "\n",
    "        if reward_func:\n",
    "            if self._test_rew_func(reward_func):\n",
    "                self.reward_func = reward_func\n",
    "        else:\n",
    "            self.reward_func = self.default_reward_func\n",
    "\n",
    "    # @property\n",
    "    # def batch_size(self):\n",
    "    #     \"\"\"Returns the batch size of the environment setup.\"\"\"\n",
    "    #     return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def default_reward_function(self):\n",
    "        \"\"\"Returns the default reward function for the given environment.\"\"\"\n",
    "        return self.default_reward_func\n",
    "\n",
    "    # @batch_size.setter\n",
    "    # def batch_size(self, batch_size):\n",
    "    #     # If batchsize change, update the corresponding dimension\n",
    "    #     self._batch_size = batch_size\n",
    "\n",
    "    def sim_paras(self, static_params_, env_state_constraints_, env_max_actions_):\n",
    "        \"\"\"Creates or updates static parameters to fit batch_size.\n",
    "\n",
    "        Creates/Updates:\n",
    "            params : Model Parameters.\n",
    "        \"\"\"\n",
    "        static_params = static_params_.copy()\n",
    "        for key, value in static_params.items():\n",
    "            if jnp.isscalar(value):\n",
    "                static_params[key]=(jnp.full((self.batch_size), value))\n",
    "                # self.static_para_dims[key] = None\n",
    "            # elif jnp.all(value == value[0]):\n",
    "            #     self.static_params[key] = jnp.full(\n",
    "            #         (self.batch_size, 1), value[0])\n",
    "            else:\n",
    "                assert len(\n",
    "                    value) == self.batch_size, f\"{key} is expected to be a scalar or a list with len(list)=batch_size\"\n",
    "                static_params[key]=(jnp.array(value))\n",
    "                # self.static_para_dims[key] = 0\n",
    "\n",
    "        env_state_constraints = env_state_constraints_.copy()\n",
    "        env_state_constraints_ar=[]\n",
    "        for key, value in env_state_constraints.items():\n",
    "            if jnp.isscalar(value):\n",
    "                env_state_constraints_ar.append(jnp.full((self.batch_size), value))\n",
    "                # self.static_para_dims[key] = None\n",
    "            # elif jnp.all(value == value[0]):\n",
    "            #     self.static_params[key] = jnp.full(\n",
    "            #         (self.batch_size, 1), value[0])\n",
    "            else:\n",
    "                assert len(\n",
    "                    value) == self.batch_size, f\"Constraint of {key} is expected to be a scalar or a list with len(list)=batch_size\"\n",
    "                env_state_constraints_ar.append(jnp.array(value))\n",
    "                # self.static_para_dims[key] = 0\n",
    "\n",
    "        env_max_actions = env_max_actions_.copy()\n",
    "        env_max_actions_ar = []\n",
    "        for key, value in env_max_actions.items():\n",
    "            if jnp.isscalar(value):\n",
    "                env_max_actions_ar.append(jnp.full((self.batch_size), value))\n",
    "                # self.static_para_dims[key] = None\n",
    "            # elif jnp.all(value == value[0]):\n",
    "            #     self.static_params[key] = jnp.full(\n",
    "            #         (self.batch_size, 1), value[0])\n",
    "            else:\n",
    "                assert len(\n",
    "                    value) == self.batch_size, f\"Constraint of {key} is expected to be a scalar or a list with len(list)=batch_size\"\n",
    "                env_max_actions_ar.append(jnp.array(value))\n",
    "                # self.static_para_dims[key] = 0\n",
    "\n",
    "        return static_params, jnp.array(env_state_constraints_ar).T, jnp.array(env_max_actions_ar).T\n",
    "\n",
    "    # def solver(self):\n",
    "    #     \"\"\"Returns the current solver of the environment setup.\"\"\"\n",
    "    #     return self._solver\n",
    "\n",
    "    # @solver.setter\n",
    "    # def solver(self, solver):\n",
    "    #     # TODO:check if solver exists in diffrax ?\n",
    "    #     self._solver = solver\n",
    "\n",
    "    def _test_rew_func(self, func):\n",
    "        \"\"\"Checks if passed reward function is compatible with given environment.\n",
    "\n",
    "        Args:\n",
    "            func(function): Reward function to test.\n",
    "\n",
    "        Returns:\n",
    "            compatible(bool): Environment compatibility.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            out = func(\n",
    "                jnp.zeros([self.batch_size, int(len(self.obs_description))]))\n",
    "        except:\n",
    "            raise Exception(\n",
    "                \"Reward function should be using obs matrix as only parameter\")\n",
    "        try:\n",
    "            if out.shape != (self.batch_size, 1):\n",
    "                raise Exception(\n",
    "                    \"Reward function should be returning vector in shape (batch_size,1)\")\n",
    "        except:\n",
    "            raise Exception(\n",
    "                \"Reward function should be returning vector in shape (batch_size,1)\")\n",
    "        return True\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def step(self, action, states):\n",
    "        \"\"\"Addtional function in step execution to enable JAX jit.\n",
    "\n",
    "        Args:\n",
    "            states(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "            action_norm(ndarray(float)): Action Matrix (shape=(batch_size,actions)).\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            Multiple Outputs:\n",
    "\n",
    "            observation(ndarray(float)): Observation/State Matrix (shape=(batch_size,states)).\n",
    "\n",
    "            reward(ndarray(float)): Amount of reward received for the last step (shape=(batch_size,1)).\n",
    "\n",
    "            terminated(bool): Flag, indicating if Agent has reached the terminal state.\n",
    "\n",
    "            truncated(ndarray(bool)): Flag, indicating if state has gone out of bounds (shape=(batch_size,states)).\n",
    "\n",
    "            {}: An empty dictionary for consistency with the OpenAi Gym interface.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        states=self.states_array_to_dict(states)\n",
    "        # ode step\n",
    "        states = jax.vmap(self._ode_exp_euler_step)(\n",
    "            states, action, self.static_params)\n",
    "        states=self.states_dict_to_array(states)\n",
    "        # observation\n",
    "        # print(states)\n",
    "        # print(self.env_state_constraints)\n",
    "        #obs = jax.vmap(self.generate_observation)(\n",
    "            #states, self.env_state_constraints)\n",
    "        # reward\n",
    "        #reward = jax.vmap(self.reward_func)(\n",
    "            #obs, action, self.env_max_actions).reshape(-1, 1)\n",
    "\n",
    "        # bound check\n",
    "        #truncated = jax.vmap(self.generate_truncated)(\n",
    "            #states, self.env_state_constraints)\n",
    "        #terminated = jax.vmap(self.generate_terminated)(states, reward)\n",
    "\n",
    "        #return obs, reward, terminated, truncated, states\n",
    "        return {},{},{},{},states\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def obs_description(self):\n",
    "        \"\"\"Returns a list of state names of all states in the observation (equal to state space).\"\"\"\n",
    "        return self.states_description\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def default_reward_func(self, obs, action):\n",
    "        \"\"\"Returns the default RewardFunction of the environment.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_observation(self, states):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return states\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_truncated(self, states):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_terminated(self, states, reward):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def _ode_exp_euler_step(self, states_norm, action_norm, state_normalizer,  action_normalizer, params):\n",
    "        \"\"\"Implementation of the system equations in the class with Explicit Euler.\n",
    "\n",
    "        Args:\n",
    "            states_norm(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "            action_norm(ndarray(float)): Action Matrix (shape=(batch_size,actions)).\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            states(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self, initial_values: jnp.ndarray = jnp.array([])):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import diffrax\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class Pendulum3(CoreEnvironment3):\n",
    "    \"\"\"\n",
    "    State Variables:\n",
    "        ``['theta', 'omega']``\n",
    "\n",
    "    Action Variable:\n",
    "        ``['torque']``\n",
    "\n",
    "    Observation Space (State Space):\n",
    "        Box(low=[-1, -1], high=[1, 1])    \n",
    "\n",
    "    Action Space:\n",
    "        Box(low=-1, high=1)\n",
    "\n",
    "    Initial State:\n",
    "        Unless chosen otherwise, theta equals 1(normalized to pi) and omega is set to zero.\n",
    "\n",
    "    Example:\n",
    "        >>> import jax\n",
    "        >>> import exciting_environments as excenvs\n",
    "        >>> \n",
    "        >>> # Create the environment\n",
    "        >>> env= excenvs.make('Pendulum-v0',batch_size=2,l=2,m=4)\n",
    "        >>> \n",
    "        >>> # Reset the environment with default initial values\n",
    "        >>> env.reset()\n",
    "        >>> \n",
    "        >>> # Sample a random action\n",
    "        >>> action = env.action_space.sample(jax.random.PRNGKey(6))\n",
    "        >>> \n",
    "        >>> # Perform step\n",
    "        >>> obs,reward,terminated,truncated,info= env.step(action)\n",
    "        >>> \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int = 8, l: float = 1, m: float = 1,  env_max_actions: list = {\"torque\": 20}, solver=diffrax.Euler(), reward_func=None, g: float = 9.81, tau: float = 1e-4, env_state_constraints: dict = {\"theta\": np.pi, \"omega\": 10}):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size(int): Number of training examples utilized in one iteration. Default: 8\n",
    "            l(float): Length of the pendulum. Default: 1\n",
    "            m(float): Mass of the pendulum tip. Default: 1\n",
    "            max_torque(float): Maximum torque that can be applied to the system as action. Default: 20 \n",
    "            reward_func(function): Reward function for training. Needs Observation-Matrix and Action as Parameters. \n",
    "                                    Default: None (default_reward_func from class) \n",
    "            g(float): Gravitational acceleration. Default: 9.81\n",
    "            tau(float): Duration of one control step in seconds. Default: 1e-4.\n",
    "            constraints(list): Constraints for state ['omega'] (list with length 1). Default: [10]\n",
    "\n",
    "        Note: l,m and max_torque can also be passed as lists with the length of the batch_size to set different parameters per batch. In addition to that constraints can also be passed as a list of lists with length 1 to set different constraints per batch.  \n",
    "        \"\"\"\n",
    "        self.env_states_name = [\"theta\", \"omega\"]\n",
    "        self.env_actions_name = [\"torque\"]\n",
    "\n",
    "        self.env_states_initials = [jnp.pi,0]\n",
    "\n",
    "        super().__init__(batch_size=batch_size, tau=tau,\n",
    "                         solver=solver, reward_func=reward_func)\n",
    "\n",
    "        self.static_params, self.env_state_constraints, self.env_max_actions = self.sim_paras(\n",
    "            {\"l\": l, \"m\": m, \"g\": g}, env_state_constraints, env_max_actions)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _ode_exp_euler_step(self, states, action, static_params):\n",
    "\n",
    "        env_states = states\n",
    "        args = (action, static_params)\n",
    "\n",
    "        def vector_field(t, y, args):\n",
    "            theta, omega = y\n",
    "            action, params = args\n",
    "            d_omega = (action[0]+params[\"l\"]*params[\"m\"]*params[\"g\"]\n",
    "                       * jnp.sin(theta)) / (params[\"m\"] * (params[\"l\"])**2)\n",
    "            d_theta = omega\n",
    "            d_y = d_theta, d_omega#[0]  # d_theta, d_omega\n",
    "            return d_y\n",
    "\n",
    "        term = diffrax.ODETerm(vector_field)\n",
    "        t0 = 0\n",
    "        t1 = self.tau\n",
    "        y0 = tuple([env_states[\"theta\"], env_states[\"omega\"]])\n",
    "        env_state = self._solver.init(term, t0, t1, y0, args)\n",
    "        y, _, _, env_state, _ = self._solver.step(\n",
    "            term, t0, t1, y0, args, env_state, made_jump=False)\n",
    "\n",
    "        theta_k1 = y[0]\n",
    "        omega_k1 = y[1]\n",
    "        theta_k1 = ((theta_k1+jnp.pi) % (2*jnp.pi))-jnp.pi\n",
    "\n",
    "        # env_states_k1 = jnp.hstack((\n",
    "        #     theta_k1,\n",
    "        #     omega_k1,\n",
    "        # ))\n",
    "        env_states_k1 = OrderedDict([(\"theta\", theta_k1), (\"omega\", omega_k1)]) #Ordered\n",
    "        # env_states_k1_norm = env_states_k1/env_state_normalizer\n",
    "\n",
    "        return env_states_k1\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def states_array_to_dict(self,states):\n",
    "        return dict({\"theta\":states[:,0],\"omega\":states[:,1]})\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def states_dict_to_array(self,states):\n",
    "        return  jnp.array([states[\"theta\"],states[\"omega\"]]).T\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def default_reward_func(self, obs, action, env_max_actions):\n",
    "        return (obs[0])**2 + 0.1*(obs[1])**2 + 0.1*(action[\"torque\"]/env_max_actions[\"torque\"])**2\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_observation(self, states, env_state_constraints):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return (jnp.array(list(states.values()))*(jnp.array(list(env_state_constraints.values())))**(-1)).T  #\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_truncated(self, states, env_state_constraints):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return jnp.abs((jnp.array(list(states.values()))/jnp.array(list(env_state_constraints.values()))).T) > 1\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_terminated(self, states, reward):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return reward == 0\n",
    "\n",
    "    @property\n",
    "    def obs_description(self):\n",
    "        return self.env_states_name\n",
    "\n",
    "    def reset(self, initial_values: jnp.ndarray = jnp.array([])):\n",
    "        # TODO\n",
    "        # if initial_values.any() != False:\n",
    "        #     assert initial_values.shape[\n",
    "        #         0] == self.batch_size, f\"number of rows is expected to be batch_size, got: {initial_values.shape[0]}\"\n",
    "        #     assert initial_values.shape[1] == len(\n",
    "        #         self.obs_description), f\"number of columns is expected to be amount obs_entries: {len(self.obs_description)}, got: {initial_values.shape[0]}\"\n",
    "        #     states = initial_values\n",
    "        # else:\n",
    "        #     states = jnp.tile(\n",
    "        #         jnp.array(self.env_state_initials), (self.batch_size, 1))\n",
    "\n",
    "        # obs = self.generate_observation(states)\n",
    "\n",
    "        return  # obs, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import chex\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from exciting_environments import spaces\n",
    "import diffrax\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class CoreEnvironment4(ABC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Core Structure of provided Environments.\n",
    "\n",
    "    State Variables:\n",
    "        Each environment has got a list of state variables that are defined by the physical system represented.\n",
    "\n",
    "        Example:\n",
    "            ``['theta', 'omega']``\n",
    "\n",
    "    Action Variable:\n",
    "        Each environment has got an action which is applied to the physical system represented.\n",
    "\n",
    "        Example:\n",
    "            ``['torque']``\n",
    "\n",
    "    Observation Space(State Space):\n",
    "        Type: Box()\n",
    "            The Observation Space is nothing but the State Space of the pyhsical system.\n",
    "            This Space is a normalized, continious, multidimensional box in [-1,1].\n",
    "\n",
    "    Action Space:\n",
    "        Type: Box()\n",
    "            The action space of the environments are the action spaces of the physical systems.\n",
    "            This Space is a continious, multidimensional box. \n",
    "\n",
    "\n",
    "    Initial State:\n",
    "        Initial state values depend on the physical system.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int, tau: float = 1e-4, solver=diffrax.Euler(), reward_func=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size(int): Number of training examples utilized in one iteration.\n",
    "            tau(float): Duration of one control step in seconds. Default: 1e-4.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self._solver = solver\n",
    "\n",
    "        if reward_func:\n",
    "            if self._test_rew_func(reward_func):\n",
    "                self.reward_func = reward_func\n",
    "        else:\n",
    "            self.reward_func = self.default_reward_func\n",
    "\n",
    "    # @property\n",
    "    # def batch_size(self):\n",
    "    #     \"\"\"Returns the batch size of the environment setup.\"\"\"\n",
    "    #     return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def default_reward_function(self):\n",
    "        \"\"\"Returns the default reward function for the given environment.\"\"\"\n",
    "        return self.default_reward_func\n",
    "\n",
    "    # @batch_size.setter\n",
    "    # def batch_size(self, batch_size):\n",
    "    #     # If batchsize change, update the corresponding dimension\n",
    "    #     self._batch_size = batch_size\n",
    "\n",
    "    def sim_paras(self, static_params_, env_state_constraints_, env_max_actions_):\n",
    "        \"\"\"Creates or updates static parameters to fit batch_size.\n",
    "\n",
    "        Creates/Updates:\n",
    "            params : Model Parameters.\n",
    "        \"\"\"\n",
    "        static_params = static_params_.copy()\n",
    "        for key, value in static_params.items():\n",
    "            if jnp.isscalar(value):\n",
    "                static_params[key]=(jnp.full((self.batch_size), value))\n",
    "                # self.static_para_dims[key] = None\n",
    "            # elif jnp.all(value == value[0]):\n",
    "            #     self.static_params[key] = jnp.full(\n",
    "            #         (self.batch_size, 1), value[0])\n",
    "            else:\n",
    "                assert len(\n",
    "                    value) == self.batch_size, f\"{key} is expected to be a scalar or a list with len(list)=batch_size\"\n",
    "                static_params[key]=(jnp.array(value))\n",
    "                # self.static_para_dims[key] = 0\n",
    "\n",
    "        env_state_constraints = env_state_constraints_.copy()\n",
    "        env_state_constraints_ar=[]\n",
    "        for key, value in env_state_constraints.items():\n",
    "            if jnp.isscalar(value):\n",
    "                env_state_constraints_ar.append(jnp.full((self.batch_size), value))\n",
    "                # self.static_para_dims[key] = None\n",
    "            # elif jnp.all(value == value[0]):\n",
    "            #     self.static_params[key] = jnp.full(\n",
    "            #         (self.batch_size, 1), value[0])\n",
    "            else:\n",
    "                assert len(\n",
    "                    value) == self.batch_size, f\"Constraint of {key} is expected to be a scalar or a list with len(list)=batch_size\"\n",
    "                env_state_constraints_ar.append(jnp.array(value))\n",
    "                # self.static_para_dims[key] = 0\n",
    "\n",
    "        env_max_actions = env_max_actions_.copy()\n",
    "        env_max_actions_ar = []\n",
    "        for key, value in env_max_actions.items():\n",
    "            if jnp.isscalar(value):\n",
    "                env_max_actions_ar.append(jnp.full((self.batch_size), value))\n",
    "                # self.static_para_dims[key] = None\n",
    "            # elif jnp.all(value == value[0]):\n",
    "            #     self.static_params[key] = jnp.full(\n",
    "            #         (self.batch_size, 1), value[0])\n",
    "            else:\n",
    "                assert len(\n",
    "                    value) == self.batch_size, f\"Constraint of {key} is expected to be a scalar or a list with len(list)=batch_size\"\n",
    "                env_max_actions_ar.append(jnp.array(value))\n",
    "                # self.static_para_dims[key] = 0\n",
    "\n",
    "        return static_params, jnp.array(env_state_constraints_ar).T, jnp.array(env_max_actions_ar).T\n",
    "\n",
    "    # def solver(self):\n",
    "    #     \"\"\"Returns the current solver of the environment setup.\"\"\"\n",
    "    #     return self._solver\n",
    "\n",
    "    # @solver.setter\n",
    "    # def solver(self, solver):\n",
    "    #     # TODO:check if solver exists in diffrax ?\n",
    "    #     self._solver = solver\n",
    "\n",
    "    def _test_rew_func(self, func):\n",
    "        \"\"\"Checks if passed reward function is compatible with given environment.\n",
    "\n",
    "        Args:\n",
    "            func(function): Reward function to test.\n",
    "\n",
    "        Returns:\n",
    "            compatible(bool): Environment compatibility.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            out = func(\n",
    "                jnp.zeros([self.batch_size, int(len(self.obs_description))]))\n",
    "        except:\n",
    "            raise Exception(\n",
    "                \"Reward function should be using obs matrix as only parameter\")\n",
    "        try:\n",
    "            if out.shape != (self.batch_size, 1):\n",
    "                raise Exception(\n",
    "                    \"Reward function should be returning vector in shape (batch_size,1)\")\n",
    "        except:\n",
    "            raise Exception(\n",
    "                \"Reward function should be returning vector in shape (batch_size,1)\")\n",
    "        return True\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def step(self, action, states):\n",
    "        \"\"\"Addtional function in step execution to enable JAX jit.\n",
    "\n",
    "        Args:\n",
    "            states(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "            action_norm(ndarray(float)): Action Matrix (shape=(batch_size,actions)).\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            Multiple Outputs:\n",
    "\n",
    "            observation(ndarray(float)): Observation/State Matrix (shape=(batch_size,states)).\n",
    "\n",
    "            reward(ndarray(float)): Amount of reward received for the last step (shape=(batch_size,1)).\n",
    "\n",
    "            terminated(bool): Flag, indicating if Agent has reached the terminal state.\n",
    "\n",
    "            truncated(ndarray(bool)): Flag, indicating if state has gone out of bounds (shape=(batch_size,states)).\n",
    "\n",
    "            {}: An empty dictionary for consistency with the OpenAi Gym interface.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # ode step\n",
    "        states = jax.vmap(self._ode_exp_euler_step)(\n",
    "            states, action, self.static_params)\n",
    "        # observation\n",
    "        # print(states)\n",
    "        # print(self.env_state_constraints)\n",
    "        #obs = jax.vmap(self.generate_observation)(\n",
    "            #states, self.env_state_constraints)\n",
    "        # reward\n",
    "        #reward = jax.vmap(self.reward_func)(\n",
    "            #obs, action, self.env_max_actions).reshape(-1, 1)\n",
    "\n",
    "        # bound check\n",
    "        #truncated = jax.vmap(self.generate_truncated)(\n",
    "            #states, self.env_state_constraints)\n",
    "        #terminated = jax.vmap(self.generate_terminated)(states, reward)\n",
    "\n",
    "        #return obs, reward, terminated, truncated, states\n",
    "        return {},{},{},{},states\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def obs_description(self):\n",
    "        \"\"\"Returns a list of state names of all states in the observation (equal to state space).\"\"\"\n",
    "        return self.states_description\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def default_reward_func(self, obs, action):\n",
    "        \"\"\"Returns the default RewardFunction of the environment.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_observation(self, states):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return states\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_truncated(self, states):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_terminated(self, states, reward):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def _ode_exp_euler_step(self, states_norm, action_norm, state_normalizer,  action_normalizer, params):\n",
    "        \"\"\"Implementation of the system equations in the class with Explicit Euler.\n",
    "\n",
    "        Args:\n",
    "            states_norm(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "            action_norm(ndarray(float)): Action Matrix (shape=(batch_size,actions)).\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            states(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self, initial_values: jnp.ndarray = jnp.array([])):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import diffrax\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class Pendulum4(CoreEnvironment4):\n",
    "    \"\"\"\n",
    "    State Variables:\n",
    "        ``['theta', 'omega']``\n",
    "\n",
    "    Action Variable:\n",
    "        ``['torque']``\n",
    "\n",
    "    Observation Space (State Space):\n",
    "        Box(low=[-1, -1], high=[1, 1])    \n",
    "\n",
    "    Action Space:\n",
    "        Box(low=-1, high=1)\n",
    "\n",
    "    Initial State:\n",
    "        Unless chosen otherwise, theta equals 1(normalized to pi) and omega is set to zero.\n",
    "\n",
    "    Example:\n",
    "        >>> import jax\n",
    "        >>> import exciting_environments as excenvs\n",
    "        >>> \n",
    "        >>> # Create the environment\n",
    "        >>> env= excenvs.make('Pendulum-v0',batch_size=2,l=2,m=4)\n",
    "        >>> \n",
    "        >>> # Reset the environment with default initial values\n",
    "        >>> env.reset()\n",
    "        >>> \n",
    "        >>> # Sample a random action\n",
    "        >>> action = env.action_space.sample(jax.random.PRNGKey(6))\n",
    "        >>> \n",
    "        >>> # Perform step\n",
    "        >>> obs,reward,terminated,truncated,info= env.step(action)\n",
    "        >>> \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int = 8, l: float = 1, m: float = 1,  env_max_actions: list = {\"torque\": 20}, solver=diffrax.Euler(), reward_func=None, g: float = 9.81, tau: float = 1e-4, env_state_constraints: dict = {\"theta\": np.pi, \"omega\": 10}):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size(int): Number of training examples utilized in one iteration. Default: 8\n",
    "            l(float): Length of the pendulum. Default: 1\n",
    "            m(float): Mass of the pendulum tip. Default: 1\n",
    "            max_torque(float): Maximum torque that can be applied to the system as action. Default: 20 \n",
    "            reward_func(function): Reward function for training. Needs Observation-Matrix and Action as Parameters. \n",
    "                                    Default: None (default_reward_func from class) \n",
    "            g(float): Gravitational acceleration. Default: 9.81\n",
    "            tau(float): Duration of one control step in seconds. Default: 1e-4.\n",
    "            constraints(list): Constraints for state ['omega'] (list with length 1). Default: [10]\n",
    "\n",
    "        Note: l,m and max_torque can also be passed as lists with the length of the batch_size to set different parameters per batch. In addition to that constraints can also be passed as a list of lists with length 1 to set different constraints per batch.  \n",
    "        \"\"\"\n",
    "        self.env_states_name = [\"theta\", \"omega\"]\n",
    "        self.env_actions_name = [\"torque\"]\n",
    "\n",
    "        self.env_states_initials = [jnp.pi,0]\n",
    "\n",
    "        super().__init__(batch_size=batch_size, tau=tau,\n",
    "                         solver=solver, reward_func=reward_func)\n",
    "\n",
    "        self.static_params, self.env_state_constraints, self.env_max_actions = self.sim_paras(\n",
    "            {\"l\": l, \"m\": m, \"g\": g}, env_state_constraints, env_max_actions)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _ode_exp_euler_step(self, states, action, static_params):\n",
    "\n",
    "        env_states = states\n",
    "        args = (action, static_params)\n",
    "\n",
    "        def vector_field(t, y, args):\n",
    "            theta, omega = y\n",
    "            action, params = args\n",
    "            d_omega = (action[0]+params[\"l\"]*params[\"m\"]*params[\"g\"]\n",
    "                       * jnp.sin(theta)) / (params[\"m\"] * (params[\"l\"])**2)\n",
    "            d_theta = omega\n",
    "            d_y = d_theta, d_omega#[0]  # d_theta, d_omega\n",
    "            return d_y\n",
    "\n",
    "        term = diffrax.ODETerm(vector_field)\n",
    "        t0 = 0\n",
    "        t1 = self.tau\n",
    "        y0 = tuple([env_states[\"theta\"], env_states[\"omega\"]])\n",
    "        env_state = self._solver.init(term, t0, t1, y0, args)\n",
    "        y, _, _, env_state, _ = self._solver.step(\n",
    "            term, t0, t1, y0, args, env_state, made_jump=False)\n",
    "\n",
    "        theta_k1 = y[0]\n",
    "        omega_k1 = y[1]\n",
    "        theta_k1 = ((theta_k1+jnp.pi) % (2*jnp.pi))-jnp.pi\n",
    "\n",
    "        # env_states_k1 = jnp.hstack((\n",
    "        #     theta_k1,\n",
    "        #     omega_k1,\n",
    "        # ))\n",
    "        env_states_k1 = OrderedDict([(\"theta\", theta_k1), (\"omega\", omega_k1)]) #Ordered\n",
    "        # env_states_k1_norm = env_states_k1/env_state_normalizer\n",
    "\n",
    "        return env_states_k1\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def states_array_to_dict(self,states):\n",
    "        return dict({\"theta\":states[:,0],\"omega\":states[:,1]})\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def states_dict_to_array(self,states):\n",
    "        return  jnp.array([states[\"theta\"],states[\"omega\"]]).T\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def default_reward_func(self, obs, action, env_max_actions):\n",
    "        return (obs[0])**2 + 0.1*(obs[1])**2 + 0.1*(action[\"torque\"]/env_max_actions[\"torque\"])**2\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_observation(self, states, env_state_constraints):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return (jnp.array(list(states.values()))*(jnp.array(list(env_state_constraints.values())))**(-1)).T  #\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_truncated(self, states, env_state_constraints):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return jnp.abs((jnp.array(list(states.values()))/jnp.array(list(env_state_constraints.values()))).T) > 1\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_terminated(self, states, reward):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return reward == 0\n",
    "\n",
    "    @property\n",
    "    def obs_description(self):\n",
    "        return self.env_states_name\n",
    "\n",
    "    def reset(self, initial_values: jnp.ndarray = jnp.array([])):\n",
    "        # TODO\n",
    "        # if initial_values.any() != False:\n",
    "        #     assert initial_values.shape[\n",
    "        #         0] == self.batch_size, f\"number of rows is expected to be batch_size, got: {initial_values.shape[0]}\"\n",
    "        #     assert initial_values.shape[1] == len(\n",
    "        #         self.obs_description), f\"number of columns is expected to be amount obs_entries: {len(self.obs_description)}, got: {initial_values.shape[0]}\"\n",
    "        #     states = initial_values\n",
    "        # else:\n",
    "        #     states = jnp.tile(\n",
    "        #         jnp.array(self.env_state_initials), (self.batch_size, 1))\n",
    "\n",
    "        # obs = self.generate_observation(states)\n",
    "\n",
    "        return  # obs, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.core import FrozenDict\n",
    "import jax_dataclasses as jdc\n",
    "@jdc.pytree_dataclass\n",
    "class States2:\n",
    "    physical_states: jax.Array\n",
    "\n",
    "    @property\n",
    "    def theta(self):\n",
    "        return self.physical_states[0]\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def omega(self):\n",
    "        return self.physical_states[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import chex\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from exciting_environments import spaces\n",
    "import diffrax\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class CoreEnvironment5(ABC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Core Structure of provided Environments.\n",
    "\n",
    "    State Variables:\n",
    "        Each environment has got a list of state variables that are defined by the physical system represented.\n",
    "\n",
    "        Example:\n",
    "            ``['theta', 'omega']``\n",
    "\n",
    "    Action Variable:\n",
    "        Each environment has got an action which is applied to the physical system represented.\n",
    "\n",
    "        Example:\n",
    "            ``['torque']``\n",
    "\n",
    "    Observation Space(State Space):\n",
    "        Type: Box()\n",
    "            The Observation Space is nothing but the State Space of the pyhsical system.\n",
    "            This Space is a normalized, continious, multidimensional box in [-1,1].\n",
    "\n",
    "    Action Space:\n",
    "        Type: Box()\n",
    "            The action space of the environments are the action spaces of the physical systems.\n",
    "            This Space is a continious, multidimensional box. \n",
    "\n",
    "\n",
    "    Initial State:\n",
    "        Initial state values depend on the physical system.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int, tau: float = 1e-4, solver=diffrax.Euler(), reward_func=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size(int): Number of training examples utilized in one iteration.\n",
    "            tau(float): Duration of one control step in seconds. Default: 1e-4.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self._solver = solver\n",
    "\n",
    "        if reward_func:\n",
    "            if self._test_rew_func(reward_func):\n",
    "                self.reward_func = reward_func\n",
    "        else:\n",
    "            self.reward_func = self.default_reward_func\n",
    "\n",
    "    # @property\n",
    "    # def batch_size(self):\n",
    "    #     \"\"\"Returns the batch size of the environment setup.\"\"\"\n",
    "    #     return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def default_reward_function(self):\n",
    "        \"\"\"Returns the default reward function for the given environment.\"\"\"\n",
    "        return self.default_reward_func\n",
    "\n",
    "    # @batch_size.setter\n",
    "    # def batch_size(self, batch_size):\n",
    "    #     # If batchsize change, update the corresponding dimension\n",
    "    #     self._batch_size = batch_size\n",
    "\n",
    "    def sim_paras(self, static_params_, env_state_constraints_, env_max_actions_):\n",
    "        \"\"\"Creates or updates static parameters to fit batch_size.\n",
    "\n",
    "        Creates/Updates:\n",
    "            params : Model Parameters.\n",
    "        \"\"\"\n",
    "        static_params = static_params_.copy()\n",
    "        for key, value in static_params.items():\n",
    "            if jnp.isscalar(value):\n",
    "                static_params[key]=(jnp.full((self.batch_size), value))\n",
    "                # self.static_para_dims[key] = None\n",
    "            # elif jnp.all(value == value[0]):\n",
    "            #     self.static_params[key] = jnp.full(\n",
    "            #         (self.batch_size, 1), value[0])\n",
    "            else:\n",
    "                assert len(\n",
    "                    value) == self.batch_size, f\"{key} is expected to be a scalar or a list with len(list)=batch_size\"\n",
    "                static_params[key]=(jnp.array(value))\n",
    "                # self.static_para_dims[key] = 0\n",
    "\n",
    "        env_state_constraints = env_state_constraints_.copy()\n",
    "        env_state_constraints_ar=[]\n",
    "        for key, value in env_state_constraints.items():\n",
    "            if jnp.isscalar(value):\n",
    "                env_state_constraints_ar.append(jnp.full((self.batch_size), value))\n",
    "                # self.static_para_dims[key] = None\n",
    "            # elif jnp.all(value == value[0]):\n",
    "            #     self.static_params[key] = jnp.full(\n",
    "            #         (self.batch_size, 1), value[0])\n",
    "            else:\n",
    "                assert len(\n",
    "                    value) == self.batch_size, f\"Constraint of {key} is expected to be a scalar or a list with len(list)=batch_size\"\n",
    "                env_state_constraints_ar.append(jnp.array(value))\n",
    "                # self.static_para_dims[key] = 0\n",
    "\n",
    "        env_max_actions = env_max_actions_.copy()\n",
    "        env_max_actions_ar = []\n",
    "        for key, value in env_max_actions.items():\n",
    "            if jnp.isscalar(value):\n",
    "                env_max_actions_ar.append(jnp.full((self.batch_size), value))\n",
    "                # self.static_para_dims[key] = None\n",
    "            # elif jnp.all(value == value[0]):\n",
    "            #     self.static_params[key] = jnp.full(\n",
    "            #         (self.batch_size, 1), value[0])\n",
    "            else:\n",
    "                assert len(\n",
    "                    value) == self.batch_size, f\"Constraint of {key} is expected to be a scalar or a list with len(list)=batch_size\"\n",
    "                env_max_actions_ar.append(jnp.array(value))\n",
    "                # self.static_para_dims[key] = 0\n",
    "\n",
    "        return static_params, jnp.array(env_state_constraints_ar).T, jnp.array(env_max_actions_ar).T\n",
    "\n",
    "    # def solver(self):\n",
    "    #     \"\"\"Returns the current solver of the environment setup.\"\"\"\n",
    "    #     return self._solver\n",
    "\n",
    "    # @solver.setter\n",
    "    # def solver(self, solver):\n",
    "    #     # TODO:check if solver exists in diffrax ?\n",
    "    #     self._solver = solver\n",
    "\n",
    "    def _test_rew_func(self, func):\n",
    "        \"\"\"Checks if passed reward function is compatible with given environment.\n",
    "\n",
    "        Args:\n",
    "            func(function): Reward function to test.\n",
    "\n",
    "        Returns:\n",
    "            compatible(bool): Environment compatibility.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            out = func(\n",
    "                jnp.zeros([self.batch_size, int(len(self.obs_description))]))\n",
    "        except:\n",
    "            raise Exception(\n",
    "                \"Reward function should be using obs matrix as only parameter\")\n",
    "        try:\n",
    "            if out.shape != (self.batch_size, 1):\n",
    "                raise Exception(\n",
    "                    \"Reward function should be returning vector in shape (batch_size,1)\")\n",
    "        except:\n",
    "            raise Exception(\n",
    "                \"Reward function should be returning vector in shape (batch_size,1)\")\n",
    "        return True\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def step(self, action, states):\n",
    "        \"\"\"Addtional function in step execution to enable JAX jit.\n",
    "\n",
    "        Args:\n",
    "            states(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "            action_norm(ndarray(float)): Action Matrix (shape=(batch_size,actions)).\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            Multiple Outputs:\n",
    "\n",
    "            observation(ndarray(float)): Observation/State Matrix (shape=(batch_size,states)).\n",
    "\n",
    "            reward(ndarray(float)): Amount of reward received for the last step (shape=(batch_size,1)).\n",
    "\n",
    "            terminated(bool): Flag, indicating if Agent has reached the terminal state.\n",
    "\n",
    "            truncated(ndarray(bool)): Flag, indicating if state has gone out of bounds (shape=(batch_size,states)).\n",
    "\n",
    "            {}: An empty dictionary for consistency with the OpenAi Gym interface.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        states=self.states_array_to_dataclass(states)\n",
    "        # ode step\n",
    "        states = jax.vmap(self._ode_exp_euler_step)(\n",
    "            states, action, self.static_params)\n",
    "        states=self.states_dataclass_to_array(states)\n",
    "        # observation\n",
    "        # print(states)\n",
    "        # print(self.env_state_constraints)\n",
    "        #obs = jax.vmap(self.generate_observation)(\n",
    "            #states, self.env_state_constraints)\n",
    "        # reward\n",
    "        #reward = jax.vmap(self.reward_func)(\n",
    "            #obs, action, self.env_max_actions).reshape(-1, 1)\n",
    "\n",
    "        # bound check\n",
    "        #truncated = jax.vmap(self.generate_truncated)(\n",
    "            #states, self.env_state_constraints)\n",
    "        #terminated = jax.vmap(self.generate_terminated)(states, reward)\n",
    "\n",
    "        #return obs, reward, terminated, truncated, states\n",
    "        return {},{},{},{},states\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def obs_description(self):\n",
    "        \"\"\"Returns a list of state names of all states in the observation (equal to state space).\"\"\"\n",
    "        return self.states_description\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def default_reward_func(self, obs, action):\n",
    "        \"\"\"Returns the default RewardFunction of the environment.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_observation(self, states):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return states\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_truncated(self, states):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_terminated(self, states, reward):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def _ode_exp_euler_step(self, states_norm, action_norm, state_normalizer,  action_normalizer, params):\n",
    "        \"\"\"Implementation of the system equations in the class with Explicit Euler.\n",
    "\n",
    "        Args:\n",
    "            states_norm(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "            action_norm(ndarray(float)): Action Matrix (shape=(batch_size,actions)).\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            states(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self, initial_values: jnp.ndarray = jnp.array([])):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import diffrax\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class Pendulum5(CoreEnvironment5):\n",
    "    \"\"\"\n",
    "    State Variables:\n",
    "        ``['theta', 'omega']``\n",
    "\n",
    "    Action Variable:\n",
    "        ``['torque']``\n",
    "\n",
    "    Observation Space (State Space):\n",
    "        Box(low=[-1, -1], high=[1, 1])    \n",
    "\n",
    "    Action Space:\n",
    "        Box(low=-1, high=1)\n",
    "\n",
    "    Initial State:\n",
    "        Unless chosen otherwise, theta equals 1(normalized to pi) and omega is set to zero.\n",
    "\n",
    "    Example:\n",
    "        >>> import jax\n",
    "        >>> import exciting_environments as excenvs\n",
    "        >>> \n",
    "        >>> # Create the environment\n",
    "        >>> env= excenvs.make('Pendulum-v0',batch_size=2,l=2,m=4)\n",
    "        >>> \n",
    "        >>> # Reset the environment with default initial values\n",
    "        >>> env.reset()\n",
    "        >>> \n",
    "        >>> # Sample a random action\n",
    "        >>> action = env.action_space.sample(jax.random.PRNGKey(6))\n",
    "        >>> \n",
    "        >>> # Perform step\n",
    "        >>> obs,reward,terminated,truncated,info= env.step(action)\n",
    "        >>> \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int = 8, l: float = 1, m: float = 1,  env_max_actions: list = {\"torque\": 20}, solver=diffrax.Euler(), reward_func=None, g: float = 9.81, tau: float = 1e-4, env_state_constraints: dict = {\"theta\": np.pi, \"omega\": 10}):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size(int): Number of training examples utilized in one iteration. Default: 8\n",
    "            l(float): Length of the pendulum. Default: 1\n",
    "            m(float): Mass of the pendulum tip. Default: 1\n",
    "            max_torque(float): Maximum torque that can be applied to the system as action. Default: 20 \n",
    "            reward_func(function): Reward function for training. Needs Observation-Matrix and Action as Parameters. \n",
    "                                    Default: None (default_reward_func from class) \n",
    "            g(float): Gravitational acceleration. Default: 9.81\n",
    "            tau(float): Duration of one control step in seconds. Default: 1e-4.\n",
    "            constraints(list): Constraints for state ['omega'] (list with length 1). Default: [10]\n",
    "\n",
    "        Note: l,m and max_torque can also be passed as lists with the length of the batch_size to set different parameters per batch. In addition to that constraints can also be passed as a list of lists with length 1 to set different constraints per batch.  \n",
    "        \"\"\"\n",
    "        self.env_states_name = [\"theta\", \"omega\"]\n",
    "        self.env_actions_name = [\"torque\"]\n",
    "\n",
    "        self.env_states_initials = [jnp.pi,0]\n",
    "\n",
    "        super().__init__(batch_size=batch_size, tau=tau,\n",
    "                         solver=solver, reward_func=reward_func)\n",
    "\n",
    "        self.static_params, self.env_state_constraints, self.env_max_actions = self.sim_paras(\n",
    "            {\"l\": l, \"m\": m, \"g\": g}, env_state_constraints, env_max_actions)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _ode_exp_euler_step(self, states, action, static_params):\n",
    "\n",
    "        env_states=states\n",
    "        args = (action, static_params)\n",
    "\n",
    "        def vector_field(t, y, args):\n",
    "            theta, omega = y\n",
    "            action, params = args\n",
    "            d_omega = (action[0]+params[\"l\"]*params[\"m\"]*params[\"g\"]\n",
    "                       * jnp.sin(theta)) / (params[\"m\"] * (params[\"l\"])**2)\n",
    "            d_theta = omega\n",
    "            d_y = d_theta, d_omega\n",
    "            return d_y\n",
    "\n",
    "        term = diffrax.ODETerm(vector_field)\n",
    "        t0 = 0\n",
    "        t1 = self.tau\n",
    "        y0 = tuple([env_states.theta, env_states.omega])\n",
    "        env_state = self._solver.init(term, t0, t1, y0, args)\n",
    "        y, _, _, env_state, _ = self._solver.step(\n",
    "            term, t0, t1, y0, args, env_state, made_jump=False)\n",
    "\n",
    "        theta_k1 = y[0]\n",
    "        omega_k1 = y[1]\n",
    "        theta_k1 = ((theta_k1+jnp.pi) % (2*jnp.pi))-jnp.pi\n",
    "\n",
    "        env_states_k1 = jnp.hstack((\n",
    "            theta_k1,\n",
    "            omega_k1,\n",
    "        ))\n",
    "\n",
    "        env_states_k1 = States2(physical_states=env_states_k1)\n",
    "\n",
    "        # env_states_k1_norm = env_states_k1/env_state_normalizer\n",
    "\n",
    "        return env_states_k1\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def states_array_to_dataclass(self,states):\n",
    "        return States2(physical_states=states)\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def states_dataclass_to_array(self,states):\n",
    "        return  states.physical_states\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def default_reward_func(self, obs, action, env_max_actions):\n",
    "        return (obs[0])**2 + 0.1*(obs[1])**2 + 0.1*(action[\"torque\"]/env_max_actions[\"torque\"])**2\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_observation(self, states, env_state_constraints):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return (jnp.array(list(states.values()))*(jnp.array(list(env_state_constraints.values())))**(-1)).T  #\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_truncated(self, states, env_state_constraints):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return jnp.abs((jnp.array(list(states.values()))/jnp.array(list(env_state_constraints.values()))).T) > 1\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_terminated(self, states, reward):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return reward == 0\n",
    "\n",
    "    @property\n",
    "    def obs_description(self):\n",
    "        return self.env_states_name\n",
    "\n",
    "    def reset(self, initial_values: jnp.ndarray = jnp.array([])):\n",
    "        # TODO\n",
    "        # if initial_values.any() != False:\n",
    "        #     assert initial_values.shape[\n",
    "        #         0] == self.batch_size, f\"number of rows is expected to be batch_size, got: {initial_values.shape[0]}\"\n",
    "        #     assert initial_values.shape[1] == len(\n",
    "        #         self.obs_description), f\"number of columns is expected to be amount obs_entries: {len(self.obs_description)}, got: {initial_values.shape[0]}\"\n",
    "        #     states = initial_values\n",
    "        # else:\n",
    "        #     states = jnp.tile(\n",
    "        #         jnp.array(self.env_state_initials), (self.batch_size, 1))\n",
    "\n",
    "        # obs = self.generate_observation(states)\n",
    "\n",
    "        return  # obs, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit_test_state_passing(env,actions,states):\n",
    "    for i in range(actions.shape[1]):\n",
    "        _,_,_,_,states= env.step(actions[:,i:i+1],states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit_test_state_random(env,actions,states):\n",
    "    for i in range(actions.shape[1]):\n",
    "        _,_,_,_,_= env.step(actions[:,i:i+1],states[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=['env'])\n",
    "def jit_timeit_test_state_passing(env,actions,states):\n",
    "    for i in range(actions.shape[1]):\n",
    "        _,_,_,_,states= env.step(actions[:,i:i+1],states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=['env'])\n",
    "def jit_timeit_test_state_random(env,actions,states):\n",
    "    for i in range(actions.shape[1]):\n",
    "        _,_,_,_,_= env.step(actions[:,i:i+1],states[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_size=10\n",
      "Array:\n",
      "168 µs ± 1.35 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "Array-Dict:\n",
      "166 µs ± 343 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "Dict:\n",
      "268 µs ± 605 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "Array-Dataclass:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Too many indices for array: 2 non-None/Ellipsis indices for dim 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeit_test_state_passing(pend_dict,act,states_dict)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArray-Dataclass:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit_test_state_passing(pend_arr_dataclass,act,states_arr)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJit-Array:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjit_timeit_test_state_passing(pend_arr,act,states_arr)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:2414\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2412\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2413\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2414\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2416\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2417\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\magics\\execution.py:1170\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m   1169\u001b[0m     number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m index\n\u001b[1;32m-> 1170\u001b[0m     time_number \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[0;32m   1172\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\magics\\execution.py:158\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    156\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[1;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m, in \u001b[0;36mtimeit_test_state_passing\u001b[1;34m(env, actions, states)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtimeit_test_state_passing\u001b[39m(env,actions,states):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(actions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m----> 3\u001b[0m         _,_,_,_,states\u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[10], line 188\u001b[0m, in \u001b[0;36mCoreEnvironment5.step\u001b[1;34m(self, action, states)\u001b[0m\n\u001b[0;32m    186\u001b[0m states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates_array_to_dataclass(states)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# ode step\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ode_exp_euler_step\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates_dataclass_to_array(states)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# observation\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# print(states)\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# print(self.env_state_constraints)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    204\u001b[0m \n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m#return obs, reward, terminated, truncated, states\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[11], line 89\u001b[0m, in \u001b[0;36mPendulum5._ode_exp_euler_step\u001b[1;34m(self, states, action, static_params)\u001b[0m\n\u001b[0;32m     87\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     88\u001b[0m t1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau\n\u001b[1;32m---> 89\u001b[0m y0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([\u001b[43menv_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta\u001b[49m, env_states\u001b[38;5;241m.\u001b[39momega])\n\u001b[0;32m     90\u001b[0m env_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solver\u001b[38;5;241m.\u001b[39minit(term, t0, t1, y0, args)\n\u001b[0;32m     91\u001b[0m y, _, _, env_state, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solver\u001b[38;5;241m.\u001b[39mstep(\n\u001b[0;32m     92\u001b[0m     term, t0, t1, y0, args, env_state, made_jump\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m, in \u001b[0;36mStates2.theta\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtheta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphysical_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Oli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:736\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m--> 736\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Oli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:349\u001b[0m, in \u001b[0;36m_getitem\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getitem\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[1;32m--> 349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax_numpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rewriting_take\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Oli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:4590\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[1;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[0;32m   4587\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mdynamic_index_in_dim(arr, idx, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   4589\u001b[0m treedef, static_idx, dynamic_idx \u001b[38;5;241m=\u001b[39m _split_index_for_jit(idx, arr\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m-> 4590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreedef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_are_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4591\u001b[0m \u001b[43m               \u001b[49m\u001b[43munique_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Oli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:4599\u001b[0m, in \u001b[0;36m_gather\u001b[1;34m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[0;32m   4596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gather\u001b[39m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[0;32m   4597\u001b[0m             unique_indices, mode, fill_value):\n\u001b[0;32m   4598\u001b[0m   idx \u001b[38;5;241m=\u001b[39m _merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)\n\u001b[1;32m-> 4599\u001b[0m   indexer \u001b[38;5;241m=\u001b[39m \u001b[43m_index_to_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shared with _scatter_update\u001b[39;00m\n\u001b[0;32m   4600\u001b[0m   y \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m   4602\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Oli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:4707\u001b[0m, in \u001b[0;36m_index_to_gather\u001b[1;34m(x_shape, idx, normalize_indices)\u001b[0m\n\u001b[0;32m   4704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_index_to_gather\u001b[39m(x_shape: Sequence[\u001b[38;5;28mint\u001b[39m], idx: Sequence[Any],\n\u001b[0;32m   4705\u001b[0m                      normalize_indices: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _Indexer:\n\u001b[0;32m   4706\u001b[0m   \u001b[38;5;66;03m# Remove ellipses and add trailing slice(None)s.\u001b[39;00m\n\u001b[1;32m-> 4707\u001b[0m   idx \u001b[38;5;241m=\u001b[39m \u001b[43m_canonicalize_tuple_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4709\u001b[0m   \u001b[38;5;66;03m# Check for scalar boolean indexing: this requires inserting extra dimensions\u001b[39;00m\n\u001b[0;32m   4710\u001b[0m   \u001b[38;5;66;03m# before performing the rest of the logic.\u001b[39;00m\n\u001b[0;32m   4711\u001b[0m   scalar_bool_dims: Sequence[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m [n \u001b[38;5;28;01mfor\u001b[39;00m n, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(idx) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i, \u001b[38;5;28mbool\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\Oli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:5027\u001b[0m, in \u001b[0;36m_canonicalize_tuple_index\u001b[1;34m(arr_ndim, idx, array_name)\u001b[0m\n\u001b[0;32m   5025\u001b[0m num_dimensions_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m (e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mEllipsis\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;28mbool\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m idx)\n\u001b[0;32m   5026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_dimensions_consumed \u001b[38;5;241m>\u001b[39m arr_ndim:\n\u001b[1;32m-> 5027\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m   5028\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many indices for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_dimensions_consumed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5029\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-None/Ellipsis indices for dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr_ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5030\u001b[0m ellipses \u001b[38;5;241m=\u001b[39m (i \u001b[38;5;28;01mfor\u001b[39;00m i, elt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(idx) \u001b[38;5;28;01mif\u001b[39;00m elt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mEllipsis\u001b[39m)\n\u001b[0;32m   5031\u001b[0m ellipsis_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(ellipses, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mIndexError\u001b[0m: Too many indices for array: 2 non-None/Ellipsis indices for dim 1."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "for batch_size in [10,100,1000,10000,100000]:\n",
    "    pend_arr= Pendulum(batch_size=batch_size,l=[np.random.uniform(0,2) for _ in range(batch_size)])\n",
    "    pend_arr_dict2= Pendulum3(batch_size=batch_size,l=[np.random.uniform(0,2) for _ in range(batch_size)])\n",
    "    pend_dict= Pendulum4(batch_size=batch_size,l=[np.random.uniform(0,2) for _ in range(batch_size)])\n",
    "    pend_arr_dataclass= Pendulum5(batch_size=batch_size,l=[np.random.uniform(0,2) for _ in range(batch_size)])\n",
    "    states_dict = OrderedDict({\"theta\":jnp.full(batch_size,np.pi),\"omega\":jnp.full(batch_size,0)})\n",
    "    states_arr = np.array(list(states_dict.values())).T\n",
    "    act=np.random.uniform(-1,1,(batch_size,20))\n",
    "    print(f\"Batch_size={batch_size}\")\n",
    "    print(\"Array:\")\n",
    "    %timeit timeit_test_state_passing(pend_arr,act,states_arr)\n",
    "    print(\"Array-Dict:\")\n",
    "    %timeit timeit_test_state_passing(pend_arr_dict2,act,states_arr)\n",
    "    print(\"Dict:\")\n",
    "    %timeit timeit_test_state_passing(pend_dict,act,states_dict)\n",
    "    print(\"Array-Dataclass:\")\n",
    "    %timeit timeit_test_state_passing(pend_arr_dataclass,act,states_arr)\n",
    "    print(\"Jit-Array:\")\n",
    "    %timeit jit_timeit_test_state_passing(pend_arr,act,states_arr)\n",
    "    print(\"Jit-Array-Dict:\")\n",
    "    %timeit jit_timeit_test_state_passing(pend_arr_dict2,act,states_arr)\n",
    "    print(\"Jit-Dict:\")\n",
    "    %timeit jit_timeit_test_state_passing(pend_dict,act,states_dict)\n",
    "    print(\"Jit-Array-Dataclass:\")\n",
    "    %timeit jit_timeit_test_state_passing(pend_arr_dataclass,act,states_arr)\n",
    "    # print(\"Dataclass:\")\n",
    "    # %timeit timeit_test_state_passing(pend_dataclass,act,states_dataclass)\n",
    "    # print(\"Hybrid-Dataclass:\")\n",
    "    # %timeit timeit_test_state_passing(pend_dataclass_hybrid,act,states_dataclass_hybrid)\n",
    "    # print(\"Jit-Array:\")\n",
    "    # %timeit jit_timeit_test_state_passing(pend_arr,act,states_arr)\n",
    "    # print(\"Jit-Dict:\")\n",
    "    # %timeit jit_timeit_test_state_passing(pend_dict,act,states_dict)\n",
    "    # print(\"Jit-Dataclass:\")\n",
    "    # %timeit jit_timeit_test_state_passing(pend_dataclass,act,states_dataclass)\n",
    "    # print(\"Jit-Hybrid-Dataclass:\")\n",
    "    # %timeit jit_timeit_test_state_passing(pend_dataclass_hybrid,act,states_dataclass_hybrid)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kontrolle der Berechnungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "l=[np.random.uniform(0,2) for _ in range(batch_size)]\n",
    "pend_arr= Pendulum(batch_size=batch_size,l=l)\n",
    "pend_arr_dict2= Pendulum3(batch_size=batch_size,l=l)\n",
    "pend_dict= Pendulum4(batch_size=batch_size,l=l)\n",
    "pend_arr_dataclass= Pendulum5(batch_size=batch_size,l=l)\n",
    "states_dict = OrderedDict({\"theta\":jnp.full(batch_size,np.pi),\"omega\":jnp.full(batch_size,0)})\n",
    "states_arr = np.array(list(states_dict.values())).T\n",
    "act=np.random.uniform(-1,1,(batch_size,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " Array([[-3.1415927e+00, -5.4698805e-05],\n",
       "        [-3.1415927e+00,  8.3036831e-04],\n",
       "        [-3.1415927e+00, -1.2508515e-07],\n",
       "        [-3.1415927e+00,  3.8730723e-05],\n",
       "        [-3.1415927e+00, -1.1539264e-05]], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pend_arr.step(act[:,0:0+1],states_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " Array([[-3.1415927e+00, -5.4698805e-05],\n",
       "        [-3.1415927e+00,  8.3036831e-04],\n",
       "        [-3.1415927e+00, -1.2508515e-07],\n",
       "        [-3.1415927e+00,  3.8730723e-05],\n",
       "        [-3.1415927e+00, -1.1539264e-05]], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pend_arr_dict2.step(act[:,0:0+1],states_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " Array([[-3.1415927e+00, -5.4698805e-05],\n",
       "        [-3.1415927e+00,  8.3036831e-04],\n",
       "        [-3.1415927e+00, -1.2508515e-07],\n",
       "        [-3.1415927e+00,  3.8730723e-05],\n",
       "        [-3.1415927e+00, -1.1539264e-05]], dtype=float32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pend_arr_dataclass.step(act[:,0:0+1],states_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " OrderedDict([('theta',\n",
       "               Array([-3.1415927, -3.1415927, -3.1415927, -3.1415927, -3.1415927],      dtype=float32, weak_type=True)),\n",
       "              ('omega',\n",
       "               Array([-5.4698805e-05,  8.3036831e-04, -1.2508515e-07,  3.8730723e-05,\n",
       "                      -1.1539264e-05], dtype=float32))]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pend_dict.step(act[:,0:0+1],states_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
