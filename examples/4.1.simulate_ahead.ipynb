{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import diffrax\n",
    "from collections import OrderedDict\n",
    "from exciting_environments import CoreEnvironment\n",
    "import jax_dataclasses as jdc\n",
    "import chex\n",
    "from jax.tree_util import tree_flatten, tree_unflatten, tree_structure\n",
    "\n",
    "\n",
    "class Pendulum(CoreEnvironment):\n",
    "    \"\"\"\n",
    "    State Variables:\n",
    "        ``['theta', 'omega']``\n",
    "\n",
    "    Action Variable:\n",
    "        ``['torque']``\n",
    "\n",
    "    Initial State:\n",
    "        Unless chosen otherwise, theta equals pi and omega is set to zero.\n",
    "\n",
    "    Example:\n",
    "        >>> import jax\n",
    "        >>> import exciting_environments as excenvs\n",
    "        >>> from exciting_environments import GymWrapper\n",
    "        >>> \n",
    "        >>> # Create the environment\n",
    "        >>> pend=excenv.Pendulum(batch_size=4,action_constraints={\"torque\":10})\n",
    "        >>> \n",
    "        >>> # Use GymWrapper for Simulation (optional)\n",
    "        >>> gym_pend=GymWrapper(env=pend)\n",
    "        >>> \n",
    "        >>> # Reset the environment with default initial values\n",
    "        >>> gym_pend.reset()\n",
    "        >>> \n",
    "        >>> # Perform step\n",
    "        >>> obs,reward,terminated,truncated,info= gym_pend.step(action=jnp.ones(4).reshape(-1,1))\n",
    "        >>> \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 8,\n",
    "        physical_constraints: dict = {\"theta\": jnp.pi, \"omega\": 10},\n",
    "        action_constraints: dict = {\"torque\": 20},\n",
    "        static_params: dict = {\"g\": 9.81, \"l\": 2, \"m\": 1},\n",
    "        solver=diffrax.Euler(),\n",
    "        reward_func=None,\n",
    "        tau: float = 1e-4,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size(int): Number of training examples utilized in one iteration. Default: 8\n",
    "            physical_constraints(jdc.pytree_dataclass): Constraints of physical states of the environment.\n",
    "                theta(float): Rotation angle. Default: jnp.pi\n",
    "                omega(float): Angular velocity. Default: 10\n",
    "            action_constraints(jdc.pytree_dataclass): Constraints of actions.\n",
    "                torque(float): Maximum torque that can be applied to the system as action. Default: 20 \n",
    "            static_params(jdc.pytree_dataclass): Parameters of environment which do not change during simulation.\n",
    "                l(float): Length of the pendulum. Default: 1\n",
    "                m(float): Mass of the pendulum tip. Default: 1\n",
    "                g(float): Gravitational acceleration. Default: 9.81\n",
    "            solver(diffrax.solver): Solver used to compute states for next step.\n",
    "            reward_func(function): Reward function for training. Needs observation vector, action and action_constraints as Parameters. \n",
    "                                    Default: None (default_reward_func from class)\n",
    "            tau(float): Duration of one control step in seconds. Default: 1e-4.\n",
    "\n",
    "        Note: Attributes of physical_constraints, action_constraints and static_params can also be passed as jnp.Array with the length of the batch_size to set different values per batch.  \n",
    "        \"\"\"\n",
    "\n",
    "        physical_constraints = self.PhysicalStates(**physical_constraints)\n",
    "        action_constraints = self.Actions(**action_constraints)\n",
    "        static_params = self.StaticParams(**static_params)\n",
    "\n",
    "        super().__init__(batch_size, physical_constraints, action_constraints, static_params, tau=tau,\n",
    "                         solver=solver, reward_func=reward_func)\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class PhysicalStates:\n",
    "        theta: jax.Array\n",
    "        omega: jax.Array\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class Optional:\n",
    "        something: jax.Array\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class StaticParams:\n",
    "        g: jax.Array\n",
    "        l: jax.Array\n",
    "        m: jax.Array\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class Actions:\n",
    "        torque: jax.Array\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def sim_ahead(self, states, actions, env_properties):\n",
    "        # ode step\n",
    "        states = self.simulate_ahaed(\n",
    "            states, actions, env_properties.static_params)\n",
    "        \n",
    "        print(jnp.array(states.ys).shape)\n",
    "        print(actions.shape)\n",
    "\n",
    "\n",
    "        # # observation\n",
    "        # obs = jax.vmap(self.generate_observation, in_axes=(1, self.in_axes_env_properties.physical_constraints))(\n",
    "        #     jnp.array(states.ys), env_properties.physical_constraints)\n",
    "        \n",
    "        # self.generate_observation(\n",
    "        #     states, env_properties.physical_constraints)\n",
    "\n",
    "        return states #obs #, states\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def vmap_sim_ahead(self, states, actions):\n",
    "\n",
    "        # vmap single operations\n",
    "        obs= jax.vmap(self.sim_ahead, in_axes=(0, 0, self.in_axes_env_properties))(\n",
    "            states, actions, self.env_properties)\n",
    "\n",
    "        return obs#, states\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def simulate_ahaed(self, states, actions, static_params):\n",
    "        \"\"\"Computes states by simulating one step.\n",
    "\n",
    "        Args:\n",
    "            states: The states from which to calculate states for the next step.\n",
    "            action: The action to apply to the environment.\n",
    "            static_params: Parameter of the environment, that do not change over time.\n",
    "\n",
    "        Returns:\n",
    "            states: The computed states after the one step simulation.\n",
    "        \"\"\"\n",
    "\n",
    "        env_states = states.physical_states\n",
    "        args = (actions, static_params)\n",
    "\n",
    "        def force(t, args):\n",
    "            actions = args\n",
    "            return actions[(t/self.tau).astype(int)]\n",
    "\n",
    "        def vector_field(t, y, args):\n",
    "            theta, omega = y\n",
    "            actions, params = args\n",
    "            d_omega = (force(t,actions)+params.l*params.m*params.g\n",
    "                       * jnp.sin(theta)) / (params.m * (params.l)**2)\n",
    "            d_theta = omega\n",
    "            d_y = d_theta, d_omega\n",
    "            return d_y\n",
    "\n",
    "        term = diffrax.ODETerm(vector_field)\n",
    "        t0 = 0\n",
    "        t1 = self.tau*actions.shape[0]\n",
    "        y0 = tuple([env_states.theta, env_states.omega])\n",
    "        saveat = diffrax.SaveAt(ts=jnp.linspace(t0, t1, actions.shape[0]+1))\n",
    "        sol = diffrax.diffeqsolve(term, self._solver, t0, t1, self.tau, y0, args=args, saveat=saveat)\n",
    "        # weiter\n",
    "        \n",
    "        # theta_k1 = y[0]\n",
    "        # omega_k1 = y[1]\n",
    "        # theta_k1 = ((theta_k1+jnp.pi) % (2*jnp.pi))-jnp.pi\n",
    "\n",
    "        # phys = self.PhysicalStates(\n",
    "        #     theta=theta_k1, omega=omega_k1)\n",
    "        # opt = None  # Optional(something=...)\n",
    "        return sol #self.States(physical_states=phys, PRNGKey=None, optional=None)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _ode_solver_step(self, states, action, static_params):\n",
    "        \"\"\"Computes states by simulating one step.\n",
    "\n",
    "        Args:\n",
    "            states: The states from which to calculate states for the next step.\n",
    "            action: The action to apply to the environment.\n",
    "            static_params: Parameter of the environment, that do not change over time.\n",
    "\n",
    "        Returns:\n",
    "            states: The computed states after the one step simulation.\n",
    "        \"\"\"\n",
    "\n",
    "        env_states = states.physical_states\n",
    "        args = (action, static_params)\n",
    "\n",
    "        def vector_field(t, y, args):\n",
    "            theta, omega = y\n",
    "            action, params = args\n",
    "            d_omega = (action[0]+params.l*params.m*params.g\n",
    "                       * jnp.sin(theta)) / (params.m * (params.l)**2)\n",
    "            d_theta = omega\n",
    "            d_y = d_theta, d_omega\n",
    "            return d_y\n",
    "\n",
    "        term = diffrax.ODETerm(vector_field)\n",
    "        t0 = 0\n",
    "        t1 = self.tau\n",
    "        y0 = tuple([env_states.theta, env_states.omega])\n",
    "        env_state = self._solver.init(term, t0, t1, y0, args)\n",
    "        y, _, _, env_state, _ = self._solver.step(\n",
    "            term, t0, t1, y0, args, env_state, made_jump=False)\n",
    "\n",
    "        theta_k1 = y[0]\n",
    "        omega_k1 = y[1]\n",
    "        theta_k1 = ((theta_k1+jnp.pi) % (2*jnp.pi))-jnp.pi\n",
    "\n",
    "        phys = self.PhysicalStates(\n",
    "            theta=theta_k1, omega=omega_k1)\n",
    "        opt = None  # Optional(something=...)\n",
    "        return self.States(physical_states=phys, PRNGKey=None, optional=None)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def init_states(self):\n",
    "        \"\"\"Returns default initial states for all batches.\"\"\"\n",
    "        phys = self.PhysicalStates(theta=jnp.full(\n",
    "            self.batch_size, jnp.pi), omega=jnp.zeros(self.batch_size))\n",
    "        opt = None  # self.Optional(something=jnp.zeros(self.batch_size))\n",
    "        return self.States(physical_states=phys, PRNGKey=None, optional=opt)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def default_reward_func(self, obs, action, action_constraints):\n",
    "        \"\"\"Returns reward for one batch.\"\"\"\n",
    "        reward = (obs[0])**2 + 0.1*(obs[1])**2 + 0.1 * \\\n",
    "            (action[0]/action_constraints.torque)**2\n",
    "        return jnp.array([reward])\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_observation(self, states, physical_constraints):\n",
    "        \"\"\"Returns observation for one batch.\"\"\"\n",
    "        obs = jnp.hstack((\n",
    "            states.physical_states.theta / physical_constraints.theta,\n",
    "            states.physical_states.omega / physical_constraints.omega,\n",
    "        ))\n",
    "        return obs\n",
    "\n",
    "    @property\n",
    "    def obs_description(self):\n",
    "        return np.array([\"theta\", \"omega\"])\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_truncated(self, states, physical_constraints):\n",
    "        \"\"\"Returns truncated information for one batch.\"\"\"\n",
    "        _states = jnp.hstack((\n",
    "            states.physical_states.theta / physical_constraints.theta,\n",
    "            states.physical_states.theta / physical_constraints.omega,\n",
    "        ))\n",
    "        return jnp.abs(_states) > 1\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_terminated(self, states, reward):\n",
    "        \"\"\"Returns terminated information for one batch.\"\"\"\n",
    "        return reward == 0\n",
    "\n",
    "    def reset(self, rng: chex.PRNGKey = None, initial_states: jdc.pytree_dataclass = None):\n",
    "        \"\"\"Resets environment to default or passed initial states.\"\"\"\n",
    "        if initial_states is not None:\n",
    "            assert tree_structure(self.init_states()) == tree_structure(\n",
    "                initial_states), f\"initial_states should have the same dataclass structure as self.init_states()\"\n",
    "            states = initial_states\n",
    "        else:\n",
    "            states = self.init_states()\n",
    "\n",
    "        obs = jax.vmap(self.generate_observation, in_axes=(0, self.in_axes_env_properties.physical_constraints))(\n",
    "            states, self.env_properties.physical_constraints)\n",
    "\n",
    "        return obs, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=Pendulum(batch_size=5,action_constraints={\"torque\":20000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoreEnvironment.States(physical_states=Pendulum.PhysicalStates(theta=Array([3.1415927, 3.1415927, 3.1415927, 3.1415927, 3.1415927],      dtype=float32, weak_type=True), omega=Array([0., 0., 0., 0., 0.], dtype=float32)), PRNGKey=None, optional=None)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states=env.init_states()\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts=jnp.ones((5,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1, 0, 1, 1, 1, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.full((5,7),jnp.array([1,0,1,1,1,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol=env.vmap_sim_ahead(states,acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.0000000e+00, 2.4999956e-05, 4.9999911e-05, 7.4999865e-05,\n",
       "       9.9999823e-05, 1.2499977e-04, 1.4999973e-04, 1.7499969e-04],      dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol.ys[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states=env.init_states()\n",
    "tree_flatten(states)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-1.000000e+00,  1.750002e-05],\n",
       "       [-1.000000e+00,  1.750002e-05],\n",
       "       [-1.000000e+00,  1.750002e-05],\n",
       "       [-1.000000e+00,  1.750002e-05],\n",
       "       [-1.000000e+00,  1.750002e-05]], dtype=float32)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs,_,_,_,states=env.vmap_step(jnp.ones(5).reshape(-1,1),states)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([7.500003e-05, 7.500003e-05, 7.500003e-05, 7.500003e-05,\n",
       "       7.500003e-05], dtype=float32)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_flatten(states)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(t/self.tau).astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
