{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import time\n",
    "import chex\n",
    "import jax\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import diffrax\n",
    "from collections import OrderedDict\n",
    "from flax.core import FrozenDict\n",
    "import jax_dataclasses as jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import chex\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from exciting_environments import spaces\n",
    "from exciting_environments.core_env import CoreEnvironment\n",
    "import diffrax\n",
    "from exciting_environments.registration import make\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class GymWrapper(ABC):\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.env = env\n",
    "        self.states = self.env.states_dataclass_to_array(self.env.init_states())\n",
    "\n",
    "        # self.action_space = spaces.Box(\n",
    "        #     low=-1.0, high=1.0, shape=(self.env.batch_size, len(list(self.env.env_max_actions.values()))), dtype=jnp.float32)\n",
    "\n",
    "        # self.env_observation_space = spaces.Box(\n",
    "        #     low=-1.0, high=1.0, shape=(self.env.batch_size, len(list(self.env.env_state_constraints.values()))), dtype=jnp.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def fromName(cls, env_id: str, **env_kwargs):\n",
    "        env = make(env_id, **env_kwargs)\n",
    "        return cls(env)\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Perform one simulation step of the environment with an action of the action space.\n",
    "\n",
    "        Args:\n",
    "            action: Action to play on the environment.\n",
    "\n",
    "        Returns:\n",
    "            Multiple Outputs:\n",
    "\n",
    "            observation(ndarray(float)): Observation/State Matrix (shape=(batch_size,states)).\n",
    "\n",
    "            reward(ndarray(float)): Amount of reward received for the last step (shape=(batch_size,1)).\n",
    "\n",
    "            terminated(bool): Flag, indicating if Agent has reached the terminal state.\n",
    "\n",
    "            truncated(ndarray(bool)): Flag, indicating if state has gone out of bounds (shape=(batch_size,states)).\n",
    "\n",
    "            {}: An empty dictionary for consistency with the OpenAi Gym interface.\n",
    "        \"\"\"\n",
    "\n",
    "        obs, reward, terminated, truncated, self.states = self.gym_step(\n",
    "            action, self.states)\n",
    "\n",
    "        return obs, reward, terminated, truncated, {}\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def gym_step(self, action, states):\n",
    "\n",
    "        # denormalize action\n",
    "        action = action*np.array(list(self.env.env_properties.action_constraints.__dict__.values())).T\n",
    "\n",
    "        states=self.env.states_array_to_dataclass(states)\n",
    "\n",
    "        obs, reward, terminated, truncated, states = self.env.vmap_step(\n",
    "            action, states)\n",
    "        \n",
    "        states=self.env.states_dataclass_to_array(states)\n",
    "\n",
    "        return obs, reward, terminated, truncated, states\n",
    "\n",
    "    def reset(self, random_key: chex.PRNGKey = None, initial_values: jnp.ndarray = jnp.array([])):\n",
    "\n",
    "        # TODO\n",
    "        if random_key != None:\n",
    "            states_mat = self.env_observation_space.sample(\n",
    "                random_key)*jnp.array(list(self.env.env_state_constraints.values())).T\n",
    "            self.states = {name: states_mat[:, idx] for name, idx in zip(\n",
    "                self.env.env_states_name, range(states_mat.shape[1]))}\n",
    "\n",
    "        else:\n",
    "            self.states = self.env.reset(initial_values=initial_values)\n",
    "\n",
    "        obs = self.env.generate_observation(\n",
    "            self.states, self.env.env_state_constraints)\n",
    "        return obs, {}\n",
    "\n",
    "    def render(self, *_, **__):\n",
    "        \"\"\"\n",
    "        Update the visualization of the environment.\n",
    "\n",
    "        NotImplemented\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"To be implemented!\")\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Called when the environment is deleted.\n",
    "\n",
    "        NotImplemented\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"To be implemented!\")\n",
    "\n",
    "    # def sim_paras(self, env_state_constraints, max_action):\n",
    "    #     \"\"\"Creates or updates parameters,variables,spaces,etc. to fit batch_size.\n",
    "\n",
    "    #     Creates/Updates:\n",
    "    #         action_space: Space for applied actions.\n",
    "    #         observation_space: Space for system states.\n",
    "    #         env_state_normalizer: Environment State normalizer to normalize and denormalize states of the environment to implement physical equations with actual values.\n",
    "    #         action_normalizer: Action normalizer to normalize and denormalize actions to implement physical equations with actual values.\n",
    "    #     \"\"\"\n",
    "    #     action_space = spaces.Box(\n",
    "    #         low=-1.0, high=1.0, shape=(self.batch_size, len(max_action)), dtype=jnp.float32)\n",
    "\n",
    "    #     env_observation_space = spaces.Box(\n",
    "    #         low=-1.0, high=1.0, shape=(self.batch_size, len(env_state_constraints)), dtype=jnp.float32)\n",
    "\n",
    "    #     return env_observation_space, action_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import chex\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from exciting_environments import spaces\n",
    "import diffrax\n",
    "import jax_dataclasses as jdc\n",
    "\n",
    "\n",
    "class CoreEnvironment(ABC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Core Structure of provided Environments.\n",
    "\n",
    "    State Variables:\n",
    "        Each environment has got a list of state variables that are defined by the physical system represented.\n",
    "\n",
    "        Example:\n",
    "            ``['theta', 'omega']``\n",
    "\n",
    "    Action Variable:\n",
    "        Each environment has got an action which is applied to the physical system represented.\n",
    "\n",
    "        Example:\n",
    "            ``['torque']``\n",
    "\n",
    "    Observation Space(State Space):\n",
    "        Type: Box()\n",
    "            The Observation Space is nothing but the State Space of the pyhsical system.\n",
    "            This Space is a normalized, continious, multidimensional box in [-1,1].\n",
    "\n",
    "    Action Space:\n",
    "        Type: Box()\n",
    "            The action space of the environments are the action spaces of the physical systems.\n",
    "            This Space is a continious, multidimensional box. \n",
    "\n",
    "\n",
    "    Initial State:\n",
    "        Initial state values depend on the physical system.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            batch_size: int, \n",
    "            physical_constraints,\n",
    "            action_constraints,\n",
    "            static_params,\n",
    "            tau: float = 1e-4, \n",
    "            solver=diffrax.Euler(),\n",
    "            reward_func=None\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size(int): Number of training examples utilized in one iteration.\n",
    "            tau(float): Duration of one control step in seconds. Default: 1e-4.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self._solver = solver\n",
    "        self.env_properties=self.EnvProperties(physical_constraints=physical_constraints,action_constraints=action_constraints,static_params=static_params)\n",
    "        if reward_func:\n",
    "            if self._test_rew_func(reward_func):\n",
    "                self.reward_func = reward_func\n",
    "        else:\n",
    "            self.reward_func = self.default_reward_func\n",
    "\n",
    "        self.in_axes_env_properties= self.create_in_axes_env_properties()\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def default_reward_function(self):\n",
    "        \"\"\"Returns the default reward function for the given environment.\"\"\"\n",
    "        return self.default_reward_func\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class PhysicalStates:\n",
    "        pass\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class Optional:\n",
    "        pass\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class StaticParams:\n",
    "        pass\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class Actions:\n",
    "        pass\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class States:\n",
    "        physical_states: jdc.pytree_dataclass #CoreEnvironment.PhysicalStates\n",
    "        PRNGKey: jax.Array\n",
    "        optional: jdc.pytree_dataclass\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class EnvProperties:\n",
    "        physical_constraints: jdc.pytree_dataclass\n",
    "        action_constraints: jdc.pytree_dataclass\n",
    "        static_params: jdc.pytree_dataclass\n",
    "\n",
    "    def create_in_axes_env_properties(self):\n",
    "        values=list(vars(self.env_properties.physical_constraints).values())\n",
    "        names=list(vars(self.env_properties.physical_constraints).keys())\n",
    "        in_axes_physical=[]\n",
    "        for v,n in zip(values,names):\n",
    "            if jnp.isscalar(v):\n",
    "                in_axes_physical.append(None)\n",
    "            else:\n",
    "                assert len(\n",
    "                    v) == self.batch_size, f\"{n} in physical_constraints is expected to be a scalar or a jnp.Array with len(jnp.Array)=batch_size\"\n",
    "                in_axes_physical.append(0)\n",
    "        \n",
    "        physical_axes=self.PhysicalStates(*tuple(in_axes_physical))\n",
    "\n",
    "        values=list(vars(self.env_properties.action_constraints).values())\n",
    "        names=list(vars(self.env_properties.action_constraints).keys())\n",
    "        in_axes_actions=[]\n",
    "        for v,n in zip(values,names):\n",
    "            if jnp.isscalar(v):\n",
    "                in_axes_actions.append(None)\n",
    "            else:\n",
    "                assert len(\n",
    "                    v) == self.batch_size, f\"{n} in action_constraints is expected to be a scalar or a jnp.Array with len(jnp.Array)=batch_size\"\n",
    "                in_axes_actions.append(0)\n",
    "        \n",
    "        action_axes=self.Actions(*tuple(in_axes_actions))\n",
    "\n",
    "        values=list(vars(self.env_properties.static_params).values())\n",
    "        names=list(vars(self.env_properties.static_params).keys())\n",
    "        in_axes_params=[]\n",
    "        for v,n in zip(values,names):\n",
    "            if jnp.isscalar(v):\n",
    "                in_axes_params.append(None)\n",
    "            else:\n",
    "                assert len(\n",
    "                    v) == self.batch_size, f\"{n} in static_params is expected to be a scalar or a jnp.Array with len(jnp.Array)=batch_size\"\n",
    "                in_axes_params.append(0)\n",
    "        \n",
    "        param_axes=self.StaticParams(*tuple(in_axes_params))\n",
    "\n",
    "        return self.EnvProperties(physical_axes,action_axes,param_axes)\n",
    "\n",
    "\n",
    "    # def _test_rew_func(self, func):\n",
    "    #     \"\"\"Checks if passed reward function is compatible with given environment.\n",
    "\n",
    "    #     Args:\n",
    "    #         func(function): Reward function to test.\n",
    "\n",
    "    #     Returns:\n",
    "    #         compatible(bool): Environment compatibility.\n",
    "    #     \"\"\"\n",
    "    #     try:\n",
    "    #         out = func(\n",
    "    #             jnp.zeros([self.batch_size, int(len(self.obs_description))]))\n",
    "    #     except:\n",
    "    #         raise Exception(\n",
    "    #             \"Reward function should be using obs matrix as only parameter\")\n",
    "    #     try:\n",
    "    #         if out.shape != (self.batch_size, 1):\n",
    "    #             raise Exception(\n",
    "    #                 \"Reward function should be returning vector in shape (batch_size,1)\")\n",
    "    #     except:\n",
    "    #         raise Exception(\n",
    "    #             \"Reward function should be returning vector in shape (batch_size,1)\")\n",
    "    #     return True\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def step(self, states, action, env_properties):\n",
    "\n",
    "        # ode step\n",
    "        states =self._ode_exp_euler_step(\n",
    "            states, action, env_properties.static_params)\n",
    "        \n",
    "        # observation\n",
    "        obs = self.generate_observation(states,env_properties.physical_constraints)\n",
    "        # reward\n",
    "        reward = self.reward_func(\n",
    "            obs, action, env_properties.action_constraints)\n",
    "\n",
    "        # bound check\n",
    "        truncated = self.generate_truncated(\n",
    "            states, env_properties.physical_constraints)\n",
    "        terminated = self.generate_terminated(states, reward)\n",
    "\n",
    "\n",
    "        return obs, reward, terminated, truncated, states\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def vmap_step(self, action, states):\n",
    "        \"\"\"Addtional function in step execution to enable JAX jit.\n",
    "\n",
    "        Args:\n",
    "            states(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "            action_norm(ndarray(float)): Action Matrix (shape=(batch_size,actions)).\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            Multiple Outputs:\n",
    "\n",
    "            observation(ndarray(float)): Observation/State Matrix (shape=(batch_size,states)).\n",
    "\n",
    "            reward(ndarray(float)): Amount of reward received for the last step (shape=(batch_size,1)).\n",
    "\n",
    "            terminated(bool): Flag, indicating if Agent has reached the terminal state.\n",
    "\n",
    "            truncated(ndarray(bool)): Flag, indicating if state has gone out of bounds (shape=(batch_size,states)).\n",
    "\n",
    "            {}: An empty dictionary for consistency with the OpenAi Gym interface.\n",
    "\n",
    "        \"\"\"\n",
    "        # vmap single operations\n",
    "        obs, reward, terminated, truncated, states = jax.vmap(self.step,in_axes=(0,0,self.in_axes_env_properties))(\n",
    "            states, action, self.env_properties)\n",
    "\n",
    "        return obs, reward, terminated, truncated, states\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def obs_description(self):\n",
    "        \"\"\"Returns a list of state names of all states in the observation (equal to state space).\"\"\"\n",
    "        return \n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def default_reward_func(self, obs, action):\n",
    "        \"\"\"Returns the default RewardFunction of the environment.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_observation(self, states):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return states\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_truncated(self, states):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def generate_terminated(self, states, reward):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    @abstractmethod\n",
    "    def _ode_exp_euler_step(self, states_norm, action_norm, state_normalizer,  action_normalizer, params):\n",
    "        \"\"\"Implementation of the system equations in the class with Explicit Euler.\n",
    "\n",
    "        Args:\n",
    "            states_norm(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "            action_norm(ndarray(float)): Action Matrix (shape=(batch_size,actions)).\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            states(ndarray(float)): State Matrix (shape=(batch_size,states)).\n",
    "\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self, initial_values: jnp.ndarray = jnp.array([])):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import diffrax\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Pendulum(CoreEnvironment):\n",
    "    \"\"\"\n",
    "    State Variables:\n",
    "        ``['theta', 'omega']``\n",
    "\n",
    "    Action Variable:\n",
    "        ``['torque']``\n",
    "\n",
    "    Observation Space (State Space):\n",
    "        Box(low=[-1, -1], high=[1, 1])    \n",
    "\n",
    "    Action Space:\n",
    "        Box(low=-1, high=1)\n",
    "\n",
    "    Initial State:\n",
    "        Unless chosen otherwise, theta equals 1(normalized to pi) and omega is set to zero.\n",
    "\n",
    "    Example:\n",
    "        >>> import jax\n",
    "        >>> import exciting_environments as excenvs\n",
    "        >>> \n",
    "        >>> # Create the environment\n",
    "        >>> env= excenvs.make('Pendulum-v0',batch_size=2,l=2,m=4)\n",
    "        >>> \n",
    "        >>> # Reset the environment with default initial values\n",
    "        >>> env.reset()\n",
    "        >>> \n",
    "        >>> # Sample a random action\n",
    "        >>> action = env.action_space.sample(jax.random.PRNGKey(6))\n",
    "        >>> \n",
    "        >>> # Perform step\n",
    "        >>> obs,reward,terminated,truncated,info= env.step(action)\n",
    "        >>> \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            batch_size: int = 8,  \n",
    "            physical_constraints: dict = {\"theta\":jnp.pi,\"omega\":10}, \n",
    "            action_constraints: dict = {\"torque\":20},\n",
    "            static_params: dict = {\"g\":9.81,\"l\":2,\"m\":1},\n",
    "            solver=diffrax.Euler(), \n",
    "            reward_func=None, \n",
    "            tau: float = 1e-4,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size(int): Number of training examples utilized in one iteration. Default: 8\n",
    "            l(float): Length of the pendulum. Default: 1\n",
    "            m(float): Mass of the pendulum tip. Default: 1\n",
    "            max_torque(float): Maximum torque that can be applied to the system as action. Default: 20 \n",
    "            reward_func(function): Reward function for training. Needs Observation-Matrix and Action as Parameters. \n",
    "                                    Default: None (default_reward_func from class) \n",
    "            g(float): Gravitational acceleration. Default: 9.81\n",
    "            tau(float): Duration of one control step in seconds. Default: 1e-4.\n",
    "            constraints(list): Constraints for state ['omega'] (list with length 1). Default: [10]\n",
    "\n",
    "        Note: l,m and max_torque can also be passed as lists with the length of the batch_size to set different parameters per batch. In addition to that constraints can also be passed as a list of lists with length 1 to set different constraints per batch.  \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        physical_constraints=self.PhysicalStates(**physical_constraints)\n",
    "        action_constraints=self.Actions(**action_constraints)\n",
    "        static_params=self.StaticParams(**static_params)\n",
    "\n",
    "        super().__init__(batch_size, physical_constraints,action_constraints,static_params,tau=tau,\n",
    "                         solver=solver, reward_func=reward_func)\n",
    "        \n",
    "    @jdc.pytree_dataclass\n",
    "    class PhysicalStates:\n",
    "        theta: jax.Array\n",
    "        omega: jax.Array\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class Optional:\n",
    "        something: jax.Array\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class StaticParams:\n",
    "        g: jax.Array\n",
    "        l: jax.Array\n",
    "        m: jax.Array\n",
    "\n",
    "    @jdc.pytree_dataclass\n",
    "    class Actions:\n",
    "        torque: jax.Array\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _ode_exp_euler_step(self, states, action, static_params):\n",
    "\n",
    "        env_states=states.physical_states\n",
    "        args = (action, static_params)\n",
    "\n",
    "        def vector_field(t, y, args):\n",
    "            theta, omega = y\n",
    "            action, params = args\n",
    "            d_omega = (action[0]+params.l*params.m*params.g\n",
    "                       * jnp.sin(theta)) / (params.m * (params.l)**2)\n",
    "            d_theta = omega\n",
    "            d_y = d_theta, d_omega\n",
    "            return d_y\n",
    "\n",
    "        term = diffrax.ODETerm(vector_field)\n",
    "        t0 = 0\n",
    "        t1 = self.tau\n",
    "        y0 = tuple([env_states.theta, env_states.omega])\n",
    "        env_state = self._solver.init(term, t0, t1, y0, args)\n",
    "        y, _, _, env_state, _ = self._solver.step(\n",
    "            term, t0, t1, y0, args, env_state, made_jump=False)\n",
    "\n",
    "        theta_k1 = y[0]\n",
    "        omega_k1 = y[1]\n",
    "        theta_k1 = ((theta_k1+jnp.pi) % (2*jnp.pi))-jnp.pi\n",
    "\n",
    "        env_states_k1 = jnp.hstack((\n",
    "            theta_k1,\n",
    "            omega_k1,\n",
    "        ))\n",
    "\n",
    "        phys=self.PhysicalStates(theta=env_states_k1[0],omega=env_states_k1[1])\n",
    "        opt = None#Optional(something=env_states_k1[0])\n",
    "        return self.States(physical_states=phys,PRNGKey=None,optional=None)\n",
    "\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def init_states(self):\n",
    "        phys=self.PhysicalStates(theta=jnp.full(self.batch_size,jnp.pi),omega=jnp.zeros(self.batch_size))\n",
    "        opt = None #self.Optional(something=jnp.zeros(self.batch_size))\n",
    "        return self.States(physical_states=phys,PRNGKey=None,optional=opt)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def states_array_to_dataclass(self,states):\n",
    "        phys=self.PhysicalStates(theta=states[:,0],omega=states[:,1])\n",
    "        opt = None #self.Optional(something=states[:,0])\n",
    "        return self.States(physical_states=phys,PRNGKey=None,optional=opt)\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def states_dataclass_to_array(self,states):\n",
    "        _states = jnp.vstack((\n",
    "            states.physical_states.theta,\n",
    "            states.physical_states.omega,\n",
    "        )).T\n",
    "        return  _states\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def default_reward_func(self, obs, action, action_constraints):\n",
    "        reward=(obs[0])**2 + 0.1*(obs[1])**2 + 0.1*(action[0]/action_constraints.torque)**2\n",
    "        return jnp.array([reward])\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_observation(self, states, physical_constraints):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        obs = jnp.hstack((\n",
    "            states.physical_states.theta / physical_constraints.theta,\n",
    "            states.physical_states.theta / physical_constraints.omega,\n",
    "        ))\n",
    "        return obs\n",
    "\n",
    "    @property\n",
    "    def obs_description(self):\n",
    "        return \n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_truncated(self, states, physical_constraints):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        _states = jnp.hstack((\n",
    "            states.physical_states.theta / physical_constraints.theta,\n",
    "            states.physical_states.theta / physical_constraints.omega,\n",
    "        ))\n",
    "        return jnp.abs(_states) > 1\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def generate_terminated(self, states, reward):\n",
    "        \"\"\"Returns states.\"\"\"\n",
    "        return reward == 0\n",
    "\n",
    "    def reset(self, initial_values: jnp.ndarray = jnp.array([])):\n",
    "        # TODO\n",
    "        # if initial_values.any() != False:\n",
    "        #     assert initial_values.shape[\n",
    "        #         0] == self.batch_size, f\"number of rows is expected to be batch_size, got: {initial_values.shape[0]}\"\n",
    "        #     assert initial_values.shape[1] == len(\n",
    "        #         self.obs_description), f\"number of columns is expected to be amount obs_entries: {len(self.obs_description)}, got: {initial_values.shape[0]}\"\n",
    "        #     states = initial_values\n",
    "        # else:\n",
    "        #     states = jnp.tile(\n",
    "        #         jnp.array(self.env_state_initials), (self.batch_size, 1))\n",
    "\n",
    "        # obs = self.generate_observation(states)\n",
    "\n",
    "        return  # obs, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=Pendulum(batch_size=5, action_constraints={\"torque\":jnp.array([10,20,30,40,60])})\n",
    "gym_pend=GymWrapper(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[-0.9999993 , -0.31415907],\n",
       "        [-0.99999857, -0.31415883],\n",
       "        [-0.9999978 , -0.3141586 ],\n",
       "        [-0.9999971 , -0.31415838],\n",
       "        [-0.9999956 , -0.3141579 ]], dtype=float32),\n",
       " Array([[1.1098682],\n",
       "        [1.1098667],\n",
       "        [1.1098652],\n",
       "        [1.1098638],\n",
       "        [1.1098608]], dtype=float32),\n",
       " Array([[False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False]], dtype=bool),\n",
       " Array([[False, False],\n",
       "        [False, False],\n",
       "        [False, False],\n",
       "        [False, False],\n",
       "        [False, False]], dtype=bool),\n",
       " {})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym_pend.step(action=jnp.ones(5).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-3.1415927e+00,  2.4999995e-04],\n",
       "       [-3.1415927e+00,  4.9999997e-04],\n",
       "       [-3.1415927e+00,  7.4999995e-04],\n",
       "       [-3.1415927e+00,  9.9999993e-04],\n",
       "       [-3.1415927e+00,  1.5000000e-03]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym_pend.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
